//! Formal verification tests for IIT/Consciousness generated by Wolfram
//!
//! These test vectors verify the mathematical correctness of
//! Integrated Information Theory (IIT) computations.
//!
//! Generated: 2025-12-06
//! Reference: Tononi et al. (2016) "Integrated Information Theory"

const EPSILON: f64 = 1e-10;

/// Wolfram-verified Shannon entropy
/// H(p) = -Σ p_i * log(p_i)
#[test]
fn test_entropy_for_phi_wolfram_verified() {
    // Uniform distribution: H = ln(n)
    let p_uniform_2 = vec![0.5, 0.5];
    let h_uniform_2 = shannon_entropy(&p_uniform_2);
    assert!(
        (h_uniform_2 - 0.6931471805599453).abs() < EPSILON,
        "H([0.5,0.5]) = {}, expected ln(2)",
        h_uniform_2
    );

    let p_uniform_4 = vec![0.25, 0.25, 0.25, 0.25];
    let h_uniform_4 = shannon_entropy(&p_uniform_4);
    assert!(
        (h_uniform_4 - 1.3862943611198906).abs() < EPSILON,
        "H([0.25,0.25,0.25,0.25]) = {}, expected ln(4)",
        h_uniform_4
    );

    // Low entropy (peaked)
    let p_peaked = vec![0.9, 0.1];
    let h_peaked = shannon_entropy(&p_peaked);
    assert!(
        (h_peaked - 0.3250829733914482).abs() < EPSILON,
        "H([0.9,0.1]) = {}, expected 0.325",
        h_peaked
    );
}

/// Wolfram-verified mutual information for Φ calculation
/// I(X;Y) = H(X) + H(Y) - H(X,Y)
#[test]
fn test_mutual_information_for_phi_wolfram_verified() {
    // Independent variables -> MI = 0
    let pxy_indep = [[0.25, 0.25], [0.25, 0.25]];
    let mi_indep = mutual_information_2x2(&pxy_indep);
    assert!(
        mi_indep.abs() < EPSILON,
        "MI(independent) = {}, expected 0",
        mi_indep
    );

    // Perfectly correlated -> MI = ln(2)
    let pxy_corr = [[0.5, 0.0], [0.0, 0.5]];
    let mi_corr = mutual_information_2x2(&pxy_corr);
    assert!(
        (mi_corr - 0.6931471805599453).abs() < EPSILON,
        "MI(correlated) = {}, expected ln(2)",
        mi_corr
    );
}

/// Verify Φ lower bound property: Φ ≥ 0
#[test]
fn test_phi_non_negative() {
    // Simulate Φ calculation: Φ = min_partition(I(A;B))
    // For any system, Φ should be non-negative

    // Independent system: Φ = 0
    let pxy = [[0.25, 0.25], [0.25, 0.25]];
    let phi_approx = mutual_information_2x2(&pxy);
    assert!(
        phi_approx >= 0.0,
        "Φ should be non-negative, got {}",
        phi_approx
    );
}

/// Wolfram-verified SOC modulation factor
/// G(σ, σ_target) = exp(-(σ - σ_target)² / (2 * variance))
#[test]
fn test_soc_modulation_wolfram_verified() {
    let sigma_target = 1.0; // Critical branching ratio
    let variance = 0.01;

    // At criticality: factor = 1.0
    let factor_crit = soc_modulation(1.0, sigma_target, variance);
    assert!(
        (factor_crit - 1.0).abs() < EPSILON,
        "SOC modulation at criticality should be 1.0, got {}",
        factor_crit
    );

    // Away from criticality: factor < 1.0
    let factor_sub = soc_modulation(0.9, sigma_target, variance);
    assert!(
        factor_sub < 1.0,
        "SOC modulation below criticality should be < 1.0, got {}",
        factor_sub
    );

    let factor_super = soc_modulation(1.1, sigma_target, variance);
    assert!(
        factor_super < 1.0,
        "SOC modulation above criticality should be < 1.0, got {}",
        factor_super
    );

    // Gaussian decay: exp(-0.5² / 0.02) = exp(-12.5) ≈ 3.73e-6
    let far_factor = soc_modulation(1.5, sigma_target, variance);
    assert!(
        far_factor < 1e-5,
        "SOC modulation far from criticality should be very small, got {}",
        far_factor
    );
}

/// Verify effective information properties
#[test]
fn test_effective_information_properties() {
    // EI should be non-negative
    let mi = mutual_information_2x2(&[[0.4, 0.1], [0.1, 0.4]]);
    assert!(
        mi >= 0.0,
        "Effective information should be non-negative"
    );

    // Higher correlation -> higher EI
    let mi_low = mutual_information_2x2(&[[0.3, 0.2], [0.2, 0.3]]);
    let mi_high = mutual_information_2x2(&[[0.45, 0.05], [0.05, 0.45]]);
    assert!(
        mi_high > mi_low,
        "Higher correlation should yield higher EI"
    );
}

// Helper functions

fn shannon_entropy(p: &[f64]) -> f64 {
    -p.iter()
        .filter(|&&pi| pi > 0.0)
        .map(|&pi| pi * pi.ln())
        .sum::<f64>()
}

fn mutual_information_2x2(pxy: &[[f64; 2]; 2]) -> f64 {
    let px = [pxy[0][0] + pxy[0][1], pxy[1][0] + pxy[1][1]];
    let py = [pxy[0][0] + pxy[1][0], pxy[0][1] + pxy[1][1]];

    let hx = -px.iter().filter(|&&p| p > 0.0).map(|&p| p * p.ln()).sum::<f64>();
    let hy = -py.iter().filter(|&&p| p > 0.0).map(|&p| p * p.ln()).sum::<f64>();

    let hxy = -pxy.iter().flat_map(|row| row.iter())
        .filter(|&&p| p > 0.0)
        .map(|&p| p * p.ln())
        .sum::<f64>();

    hx + hy - hxy
}

fn soc_modulation(sigma: f64, sigma_target: f64, variance: f64) -> f64 {
    let deviation = sigma - sigma_target;
    (-(deviation * deviation) / (2.0 * variance)).exp()
}
