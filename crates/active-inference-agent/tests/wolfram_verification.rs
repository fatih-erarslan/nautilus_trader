//! Formal verification tests for Active Inference generated by Wolfram
//!
//! These test vectors verify the mathematical correctness of
//! variational free energy and active inference computations.
//!
//! Generated: 2025-12-06
//! Reference: Friston (2010) "The free-energy principle: a unified brain theory?"

const EPSILON: f64 = 1e-10;

/// Wolfram-verified KL divergence
/// KL(P||Q) = Σ P_i * log(P_i / Q_i)
#[test]
fn test_kl_divergence_wolfram_verified() {
    // Test case 1: identical distributions -> KL = 0
    let p1 = vec![0.5, 0.5];
    let q1 = vec![0.5, 0.5];
    let kl1 = kl_divergence(&p1, &q1);
    assert!(
        kl1.abs() < EPSILON,
        "KL([0.5,0.5]||[0.5,0.5]) = {}, expected 0",
        kl1
    );

    // Test case 2: KL([0.9,0.1]||[0.5,0.5]) = 0.3680642071684971
    let p2 = vec![0.9, 0.1];
    let q2 = vec![0.5, 0.5];
    let kl2 = kl_divergence(&p2, &q2);
    assert!(
        (kl2 - 0.3680642071684971).abs() < 1e-8,
        "KL([0.9,0.1]||[0.5,0.5]) = {}, expected 0.3680642071684971",
        kl2
    );

    // Test case 3: 3-outcome distribution
    let p3 = vec![0.7, 0.2, 0.1];
    let q3 = vec![0.33, 0.33, 0.34];
    let kl3 = kl_divergence(&p3, &q3);
    assert!(
        (kl3 - 0.3038587756633056).abs() < 1e-8,
        "KL 3-outcome = {}, expected 0.3038587756633056",
        kl3
    );
}

/// Wolfram-verified Shannon entropy
/// H(p) = -Σ p_i * log(p_i)
#[test]
fn test_shannon_entropy_wolfram_verified() {
    // Test case 1: uniform binary -> H = ln(2) = 0.6931471805599453
    let p1 = vec![0.5, 0.5];
    let h1 = shannon_entropy(&p1);
    assert!(
        (h1 - 0.6931471805599453).abs() < 1e-10,
        "H([0.5,0.5]) = {}, expected ln(2)",
        h1
    );

    // Test case 2: uniform 4-outcome -> H = ln(4) = 1.3862943611198906
    let p2 = vec![0.25, 0.25, 0.25, 0.25];
    let h2 = shannon_entropy(&p2);
    assert!(
        (h2 - 1.3862943611198906).abs() < 1e-10,
        "H([0.25,0.25,0.25,0.25]) = {}, expected ln(4)",
        h2
    );

    // Test case 3: skewed distribution -> H = 0.3250829733914482
    let p3 = vec![0.9, 0.1];
    let h3 = shannon_entropy(&p3);
    assert!(
        (h3 - 0.3250829733914482).abs() < 1e-10,
        "H([0.9,0.1]) = {}, expected 0.3250829733914482",
        h3
    );
}

/// Wolfram-verified softmax
/// p_i = exp(x_i) / Σ exp(x_j)
#[test]
fn test_softmax_wolfram_verified() {
    // Test case 1: [0,0] -> [0.5, 0.5]
    let result1 = softmax(&[0.0, 0.0]);
    assert!((result1[0] - 0.5).abs() < EPSILON);
    assert!((result1[1] - 0.5).abs() < EPSILON);

    // Test case 2: [1,2,3] -> [0.0900, 0.2447, 0.6652]
    let result2 = softmax(&[1.0, 2.0, 3.0]);
    assert!(
        (result2[0] - 0.09003057317038046).abs() < 1e-8,
        "softmax([1,2,3])[0] = {}, expected 0.0900",
        result2[0]
    );
    assert!(
        (result2[1] - 0.24472847105479764).abs() < 1e-8,
        "softmax([1,2,3])[1] = {}, expected 0.2447",
        result2[1]
    );
    assert!(
        (result2[2] - 0.6652409557748219).abs() < 1e-8,
        "softmax([1,2,3])[2] = {}, expected 0.6652",
        result2[2]
    );

    // Test case 3: numerical stability with large values
    let result3 = softmax(&[100.0, 100.0, 100.0]);
    for p in &result3 {
        assert!(
            (p - 0.3333333333333333).abs() < 1e-8,
            "softmax([100,100,100]) should be uniform"
        );
    }
}

/// Verify softmax probabilities sum to 1
#[test]
fn test_softmax_sums_to_one() {
    let test_cases = vec![
        vec![0.0, 0.0],
        vec![1.0, 2.0, 3.0],
        vec![-10.0, 0.0, 10.0],
        vec![100.0, 100.0, 100.0],
    ];

    for x in test_cases {
        let probs = softmax(&x);
        let sum: f64 = probs.iter().sum();
        assert!(
            (sum - 1.0).abs() < EPSILON,
            "Softmax sums to {} for {:?}",
            sum,
            x
        );
    }
}

// Helper functions (should match the crate's implementations)

fn kl_divergence(p: &[f64], q: &[f64]) -> f64 {
    p.iter()
        .zip(q.iter())
        .filter(|(&pi, &qi)| pi > 0.0 && qi > 0.0)
        .map(|(&pi, &qi)| pi * (pi / qi).ln())
        .sum()
}

fn shannon_entropy(p: &[f64]) -> f64 {
    -p.iter()
        .filter(|&&pi| pi > 0.0)
        .map(|&pi| pi * pi.ln())
        .sum::<f64>()
}

fn softmax(x: &[f64]) -> Vec<f64> {
    let max_x = x.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    let exp_x: Vec<f64> = x.iter().map(|&xi| (xi - max_x).exp()).collect();
    let sum_exp: f64 = exp_x.iter().sum();
    exp_x.iter().map(|&e| e / sum_exp).collect()
}
