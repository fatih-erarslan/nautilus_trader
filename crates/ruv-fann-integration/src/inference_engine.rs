//! # High-Performance Inference Engine\n//!\n//! This module provides a high-performance inference engine that integrates\n//! ruv-FANN neural networks with Nautilus Trader's real-time data pipeline.\n\nuse crate::error::{IntegrationError, Result};\nuse crate::ffi::{RuvFannNetwork, NetworkConfig, PredictionResult};\nuse crate::config::IntegrationConfig;\nuse crate::metrics::MetricsCollector;\nuse anyhow::anyhow;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::{RwLock, Semaphore};\nuse tokio::time::{Duration, Instant};\nuse tracing::{debug, error, info, warn};\n\n/// Trading data input for neural inference\n#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]\npub struct TradingData {\n    pub timestamp: chrono::DateTime<chrono::Utc>,\n    pub symbol: String,\n    pub features: Vec<f32>,\n    pub metadata: HashMap<String, serde_json::Value>,\n}\n\n/// Neural network prediction output\n#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]\npub struct NeuralPrediction {\n    pub timestamp: chrono::DateTime<chrono::Utc>,\n    pub symbol: String,\n    pub predictions: Vec<f32>,\n    pub confidence: f32,\n    pub processing_time_ms: f64,\n    pub model_id: String,\n    pub features_used: usize,\n}\n\n/// Model performance statistics\n#[derive(Debug, Clone)]\npub struct ModelStats {\n    pub total_predictions: u64,\n    pub average_latency_ms: f64,\n    pub success_rate: f64,\n    pub last_updated: Instant,\n    pub accuracy_metrics: HashMap<String, f64>,\n}\n\n/// Neural model container\n#[derive(Debug)]\nstruct ModelContainer {\n    network: Arc<RuvFannNetwork>,\n    model_id: String,\n    config: NetworkConfig,\n    stats: Arc<RwLock<ModelStats>>,\n    last_used: Arc<RwLock<Instant>>,\n}\n\n/// High-performance inference engine\npub struct InferenceEngine {\n    models: Arc<RwLock<HashMap<String, ModelContainer>>>,\n    config: IntegrationConfig,\n    metrics: Arc<MetricsCollector>,\n    inference_semaphore: Arc<Semaphore>,\n    prediction_cache: Arc<RwLock<HashMap<String, (NeuralPrediction, Instant)>>>,\n    cache_ttl: Duration,\n}\n\nimpl InferenceEngine {\n    /// Create a new inference engine\n    pub async fn new(config: &IntegrationConfig) -> Result<Self> {\n        let max_concurrent_inferences = config.max_concurrent_inferences.unwrap_or(100);\n        let cache_ttl = Duration::from_millis(config.cache_ttl_ms.unwrap_or(1000));\n        \n        Ok(Self {\n            models: Arc::new(RwLock::new(HashMap::new())),\n            config: config.clone(),\n            metrics: Arc::new(MetricsCollector::new()),\n            inference_semaphore: Arc::new(Semaphore::new(max_concurrent_inferences)),\n            prediction_cache: Arc::new(RwLock::new(HashMap::new())),\n            cache_ttl,\n        })\n    }\n\n    /// Initialize the inference engine\n    pub async fn initialize(&mut self) -> Result<()> {\n        info!(\"Initializing inference engine...\");\n        \n        // Load pre-trained models if specified\n        if let Some(model_paths) = &self.config.model_paths {\n            for (model_id, path) in model_paths {\n                self.load_model(model_id, path).await?;\n            }\n        }\n        \n        // Start background tasks\n        self.start_cache_cleanup_task().await;\n        self.start_model_stats_task().await;\n        \n        info!(\"Inference engine initialized successfully\");\n        Ok(())\n    }\n\n    /// Load a neural network model\n    pub async fn load_model(&self, model_id: &str, model_path: &str) -> Result<()> {\n        info!(\"Loading model {} from {}\", model_id, model_path);\n        \n        // Load the network from file\n        let network = RuvFannNetwork::load(model_path)\n            .map_err(|e| IntegrationError::ModelLoadFailed(format!(\n                \"Failed to load model {}: {}\", model_id, e\n            )))?;\n        \n        // Create model container\n        let container = ModelContainer {\n            network: Arc::new(network),\n            model_id: model_id.to_string(),\n            config: NetworkConfig {\n                input_size: 0, // Will be determined from network\n                hidden_layers: std::ptr::null(),\n                hidden_count: 0,\n                output_size: 0,\n                learning_rate: 0.1,\n                activation_function: 3, // Sigmoid\n                training_algorithm: 2,  // RPROP\n                enable_gpu: if self.config.enable_gpu { 1 } else { 0 },\n                enable_simd: if self.config.enable_simd { 1 } else { 0 },\n            },\n            stats: Arc::new(RwLock::new(ModelStats {\n                total_predictions: 0,\n                average_latency_ms: 0.0,\n                success_rate: 1.0,\n                last_updated: Instant::now(),\n                accuracy_metrics: HashMap::new(),\n            })),\n            last_used: Arc::new(RwLock::new(Instant::now())),\n        };\n        \n        // Store the model\n        self.models.write().await.insert(model_id.to_string(), container);\n        \n        info!(\"Model {} loaded successfully\", model_id);\n        Ok(())\n    }\n\n    /// Create a new neural network model\n    pub async fn create_model(&self, model_id: &str, config: NetworkConfig) -> Result<()> {\n        info!(\"Creating new model {} with configuration {:?}\", model_id, config);\n        \n        let network = RuvFannNetwork::new(config.clone())\n            .map_err(|e| IntegrationError::ModelCreationFailed(format!(\n                \"Failed to create model {}: {}\", model_id, e\n            )))?;\n        \n        let container = ModelContainer {\n            network: Arc::new(network),\n            model_id: model_id.to_string(),\n            config,\n            stats: Arc::new(RwLock::new(ModelStats {\n                total_predictions: 0,\n                average_latency_ms: 0.0,\n                success_rate: 1.0,\n                last_updated: Instant::now(),\n                accuracy_metrics: HashMap::new(),\n            })),\n            last_used: Arc::new(RwLock::new(Instant::now())),\n        };\n        \n        self.models.write().await.insert(model_id.to_string(), container);\n        \n        info!(\"Model {} created successfully\", model_id);\n        Ok(())\n    }\n\n    /// Run inference on trading data\n    pub async fn predict(&self, data: &TradingData) -> Result<NeuralPrediction> {\n        let start_time = Instant::now();\n        \n        // Check cache first\n        let cache_key = format!(\"{}_{}\", data.symbol, data.timestamp.timestamp_millis());\n        if let Some((prediction, cached_at)) = self.prediction_cache.read().await.get(&cache_key) {\n            if start_time.duration_since(*cached_at) < self.cache_ttl {\n                debug!(\"Returning cached prediction for {}\", data.symbol);\n                return Ok(prediction.clone());\n            }\n        }\n        \n        // Acquire semaphore to limit concurrent inferences\n        let _permit = self.inference_semaphore.acquire().await\n            .map_err(|e| IntegrationError::InferenceFailed(format!(\"Semaphore error: {}\", e)))?;\n        \n        // Find appropriate model (simplified - in practice, would use model selection logic)\n        let model_id = self.select_model_for_symbol(&data.symbol).await?;\n        \n        let models = self.models.read().await;\n        let model_container = models.get(&model_id)\n            .ok_or_else(|| IntegrationError::ModelNotFound(model_id.clone()))?;\n        \n        // Update last used timestamp\n        *model_container.last_used.write().await = start_time;\n        \n        // Run inference\n        let prediction_result = model_container.network.predict(&data.features)\n            .map_err(|e| IntegrationError::InferenceFailed(format!(\"Prediction failed: {}\", e)))?;\n        \n        let processing_time = start_time.elapsed();\n        \n        // Create prediction result\n        let prediction = NeuralPrediction {\n            timestamp: chrono::Utc::now(),\n            symbol: data.symbol.clone(),\n            predictions: unsafe {\n                std::slice::from_raw_parts(\n                    prediction_result.outputs,\n                    prediction_result.output_count as usize\n                ).to_vec()\n            },\n            confidence: prediction_result.confidence,\n            processing_time_ms: processing_time.as_secs_f64() * 1000.0,\n            model_id,\n            features_used: data.features.len(),\n        };\n        \n        // Update model statistics\n        self.update_model_stats(&model_container, processing_time).await;\n        \n        // Cache the prediction\n        self.prediction_cache.write().await.insert(cache_key, (prediction.clone(), start_time));\n        \n        // Record metrics\n        self.metrics.record_inference_time(processing_time);\n        self.metrics.record_prediction(&prediction);\n        \n        debug!(\"Inference completed for {} in {:?}\", data.symbol, processing_time);\n        Ok(prediction)\n    }\n\n    /// Predict for multiple symbols in parallel\n    pub async fn predict_batch(&self, data_batch: &[TradingData]) -> Result<Vec<NeuralPrediction>> {\n        let start_time = Instant::now();\n        \n        // Process predictions in parallel\n        let prediction_futures = data_batch.iter().map(|data| self.predict(data));\n        \n        let results = futures::future::try_join_all(prediction_futures).await?;\n        \n        let batch_time = start_time.elapsed();\n        info!(\"Batch prediction completed: {} items in {:?}\", data_batch.len(), batch_time);\n        \n        Ok(results)\n    }\n\n    /// Get model statistics\n    pub async fn get_model_stats(&self, model_id: &str) -> Result<ModelStats> {\n        let models = self.models.read().await;\n        let model_container = models.get(model_id)\n            .ok_or_else(|| IntegrationError::ModelNotFound(model_id.to_string()))?;\n        \n        Ok(model_container.stats.read().await.clone())\n    }\n\n    /// List all loaded models\n    pub async fn list_models(&self) -> Vec<String> {\n        self.models.read().await.keys().cloned().collect()\n    }\n\n    /// Remove a model from memory\n    pub async fn unload_model(&self, model_id: &str) -> Result<()> {\n        let mut models = self.models.write().await;\n        if models.remove(model_id).is_some() {\n            info!(\"Model {} unloaded\", model_id);\n            Ok(())\n        } else {\n            Err(IntegrationError::ModelNotFound(model_id.to_string()))\n        }\n    }\n\n    /// Shutdown the inference engine\n    pub async fn shutdown(&mut self) -> Result<()> {\n        info!(\"Shutting down inference engine...\");\n        \n        // Clear models\n        self.models.write().await.clear();\n        \n        // Clear cache\n        self.prediction_cache.write().await.clear();\n        \n        info!(\"Inference engine shutdown completed\");\n        Ok(())\n    }\n\n    // Private helper methods\n    \n    async fn select_model_for_symbol(&self, symbol: &str) -> Result<String> {\n        // Simplified model selection - in practice, would use sophisticated logic\n        let models = self.models.read().await;\n        \n        if models.is_empty() {\n            return Err(IntegrationError::NoModelsLoaded);\n        }\n        \n        // For now, just return the first available model\n        // In practice, would select based on symbol, market conditions, etc.\n        Ok(models.keys().next().unwrap().clone())\n    }\n    \n    async fn update_model_stats(&self, model_container: &ModelContainer, processing_time: Duration) {\n        let mut stats = model_container.stats.write().await;\n        stats.total_predictions += 1;\n        \n        // Update rolling average latency\n        let new_latency = processing_time.as_secs_f64() * 1000.0;\n        stats.average_latency_ms = (stats.average_latency_ms * 0.9) + (new_latency * 0.1);\n        stats.last_updated = Instant::now();\n    }\n    \n    async fn start_cache_cleanup_task(&self) {\n        let cache = Arc::clone(&self.prediction_cache);\n        let ttl = self.cache_ttl;\n        \n        tokio::spawn(async move {\n            let mut interval = tokio::time::interval(Duration::from_secs(60));\n            \n            loop {\n                interval.tick().await;\n                \n                let now = Instant::now();\n                let mut cache_write = cache.write().await;\n                \n                // Remove expired entries\n                cache_write.retain(|_, (_, cached_at)| {\n                    now.duration_since(*cached_at) < ttl\n                });\n                \n                debug!(\"Cache cleanup completed, {} entries remaining\", cache_write.len());\n            }\n        });\n    }\n    \n    async fn start_model_stats_task(&self) {\n        let models = Arc::clone(&self.models);\n        \n        tokio::spawn(async move {\n            let mut interval = tokio::time::interval(Duration::from_secs(30));\n            \n            loop {\n                interval.tick().await;\n                \n                let models_read = models.read().await;\n                for (model_id, container) in models_read.iter() {\n                    let stats = container.stats.read().await;\n                    debug!(\n                        \"Model {} stats: {} predictions, {:.2}ms avg latency\",\n                        model_id,\n                        stats.total_predictions,\n                        stats.average_latency_ms\n                    );\n                }\n            }\n        });\n    }\n}\n\n// Thread safety\nunsafe impl Send for InferenceEngine {}\nunsafe impl Sync for InferenceEngine {}\n