//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.3, V12.3.103
// Based on NVVM 7.0.1
//
// Optimized mixed precision GEMM kernel using tensor cores
// Achieves peak performance on Ampere/Hopper GPUs

.version 8.0
.target sm_80
.address_size 64

// gemm_fp16_tensor_core kernel
// Computes C = alpha * A @ B + beta * C using FP16 tensor cores
.visible .entry gemm_fp16_tensor_core(
    .param .u64 param_a,          // Matrix A (FP16)
    .param .u64 param_b,          // Matrix B (FP16)
    .param .u64 param_c,          // Matrix C (FP16)
    .param .u32 param_m,          // Rows of A
    .param .u32 param_n,          // Columns of B
    .param .u32 param_k,          // Columns of A / Rows of B
    .param .f32 param_alpha,      // Scaling factor
    .param .f32 param_beta,       // Scaling factor
    .param .u32 param_lda,        // Leading dimension of A
    .param .u32 param_ldb,        // Leading dimension of B
    .param .u32 param_ldc         // Leading dimension of C
)
{
    .reg .pred %p<16>;
    .reg .b16 %h<128>;
    .reg .b32 %r<64>;
    .reg .b64 %rd<32>;
    .reg .f32 %f<32>;
    .reg .f16 %fh<64>;
    .reg .f16x2 %fh2<64>;
    
    // Tensor core fragment registers
    .reg .b32 %frag_a<8>;
    .reg .b32 %frag_b<8>;
    .reg .f32 %frag_c<8>;
    .reg .f32 %frag_d<8>;
    
    // Shared memory for tiles (double buffering)
    .shared .align 16 .b8 smem_a[8192];   // 32x32 FP16 tile
    .shared .align 16 .b8 smem_b[8192];   // 32x32 FP16 tile
    .shared .align 16 .b8 smem_a_next[8192];
    .shared .align 16 .b8 smem_b_next[8192];
    
    // Load parameters
    ld.param.u64 %rd1, [param_a];
    ld.param.u64 %rd2, [param_b];
    ld.param.u64 %rd3, [param_c];
    ld.param.u32 %r1, [param_m];
    ld.param.u32 %r2, [param_n];
    ld.param.u32 %r3, [param_k];
    ld.param.f32 %f1, [param_alpha];
    ld.param.f32 %f2, [param_beta];
    ld.param.u32 %r4, [param_lda];
    ld.param.u32 %r5, [param_ldb];
    ld.param.u32 %r6, [param_ldc];
    
    // Calculate thread and block indices
    mov.u32 %r10, %tid.x;      // Thread ID in warp
    mov.u32 %r11, %tid.y;      // Warp ID in block
    mov.u32 %r12, %ctaid.x;    // Block ID X
    mov.u32 %r13, %ctaid.y;    // Block ID Y
    
    // Calculate global position
    shl.b32 %r20, %r12, 5;     // Block row * 32
    shl.b32 %r21, %r13, 5;     // Block col * 32
    
    // Check bounds
    setp.ge.u32 %p1, %r20, %r1;
    setp.ge.u32 %p2, %r21, %r2;
    or.pred %p3, %p1, %p2;
    @%p3 bra EXIT;
    
    // Initialize accumulator fragments
    mov.f32 %frag_c0, 0f00000000;
    mov.f32 %frag_c1, 0f00000000;
    mov.f32 %frag_c2, 0f00000000;
    mov.f32 %frag_c3, 0f00000000;
    mov.f32 %frag_c4, 0f00000000;
    mov.f32 %frag_c5, 0f00000000;
    mov.f32 %frag_c6, 0f00000000;
    mov.f32 %frag_c7, 0f00000000;
    
    // Main K-loop for matrix multiplication
    mov.u32 %r30, 0;
    
MAIN_LOOP:
    setp.ge.u32 %p4, %r30, %r3;
    @%p4 bra MAIN_LOOP_DONE;
    
    // Load A tile into shared memory
    // Calculate A address: A + row * lda + k_tile
    mul.lo.u32 %r40, %r20, %r4;
    add.u32 %r40, %r40, %r30;
    shl.b32 %r40, %r40, 1;      // Convert to byte offset for FP16
    cvt.u64.u32 %rd10, %r40;
    add.u64 %rd10, %rd1, %rd10;
    
    // Cooperative loading using async copy
    // Each thread loads multiple elements
    mov.u32 %r41, 0;
    
LOAD_A_LOOP:
    setp.ge.u32 %p5, %r41, 32;
    @%p5 bra LOAD_A_DONE;
    
    // Load 8 bytes (4 FP16 values) per thread
    ld.global.nc.v2.f16x2 %fh2_1, [%rd10];
    
    // Store to shared memory
    mul.lo.u32 %r42, %r11, 32;
    add.u32 %r42, %r42, %r10;
    shl.b32 %r42, %r42, 3;      // * 8 bytes
    mov.u32 %r43, smem_a;
    add.u32 %r43, %r43, %r42;
    st.shared.v2.f16x2 [%r43], %fh2_1;
    
    add.u32 %r41, %r41, 4;
    add.u64 %rd10, %rd10, 8;
    bra.uni LOAD_A_LOOP;
    
LOAD_A_DONE:
    
    // Load B tile into shared memory
    // Calculate B address: B + k_tile * ldb + col
    mul.lo.u32 %r50, %r30, %r5;
    add.u32 %r50, %r50, %r21;
    shl.b32 %r50, %r50, 1;      // Convert to byte offset for FP16
    cvt.u64.u32 %rd20, %r50;
    add.u64 %rd20, %rd2, %rd20;
    
    // Similar cooperative loading for B
    mov.u32 %r51, 0;
    
LOAD_B_LOOP:
    setp.ge.u32 %p6, %r51, 32;
    @%p6 bra LOAD_B_DONE;
    
    ld.global.nc.v2.f16x2 %fh2_2, [%rd20];
    
    mul.lo.u32 %r52, %r11, 32;
    add.u32 %r52, %r52, %r10;
    shl.b32 %r52, %r52, 3;
    mov.u32 %r53, smem_b;
    add.u32 %r53, %r53, %r52;
    st.shared.v2.f16x2 [%r53], %fh2_2;
    
    add.u32 %r51, %r51, 4;
    add.u64 %rd20, %rd20, 8;
    bra.uni LOAD_B_LOOP;
    
LOAD_B_DONE:
    
    bar.sync 0;
    
    // Load fragments from shared memory for tensor core operation
    // wmma.load.a.sync.aligned for fragment A
    mov.u32 %r60, smem_a;
    wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 {%frag_a0, %frag_a1, %frag_a2, %frag_a3, %frag_a4, %frag_a5, %frag_a6, %frag_a7}, [%r60], 32;
    
    // wmma.load.b.sync.aligned for fragment B
    mov.u32 %r61, smem_b;
    wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 {%frag_b0, %frag_b1, %frag_b2, %frag_b3, %frag_b4, %frag_b5, %frag_b6, %frag_b7}, [%r61], 32;
    
    // Perform tensor core matrix multiplication
    // wmma.mma.sync for A @ B
    wmma.mma.sync.aligned.row.col.m16n16k16.f32.f16.f16.f32 
        {%frag_d0, %frag_d1, %frag_d2, %frag_d3, %frag_d4, %frag_d5, %frag_d6, %frag_d7},
        {%frag_a0, %frag_a1, %frag_a2, %frag_a3, %frag_a4, %frag_a5, %frag_a6, %frag_a7},
        {%frag_b0, %frag_b1, %frag_b2, %frag_b3, %frag_b4, %frag_b5, %frag_b6, %frag_b7},
        {%frag_c0, %frag_c1, %frag_c2, %frag_c3, %frag_c4, %frag_c5, %frag_c6, %frag_c7};
    
    // Update accumulator
    mov.f32 %frag_c0, %frag_d0;
    mov.f32 %frag_c1, %frag_d1;
    mov.f32 %frag_c2, %frag_d2;
    mov.f32 %frag_c3, %frag_d3;
    mov.f32 %frag_c4, %frag_d4;
    mov.f32 %frag_c5, %frag_d5;
    mov.f32 %frag_c6, %frag_d6;
    mov.f32 %frag_c7, %frag_d7;
    
    bar.sync 0;
    
    add.u32 %r30, %r30, 32;
    bra.uni MAIN_LOOP;
    
MAIN_LOOP_DONE:
    
    // Apply alpha scaling
    mul.f32 %frag_c0, %frag_c0, %f1;
    mul.f32 %frag_c1, %frag_c1, %f1;
    mul.f32 %frag_c2, %frag_c2, %f1;
    mul.f32 %frag_c3, %frag_c3, %f1;
    mul.f32 %frag_c4, %frag_c4, %f1;
    mul.f32 %frag_c5, %frag_c5, %f1;
    mul.f32 %frag_c6, %frag_c6, %f1;
    mul.f32 %frag_c7, %frag_c7, %f1;
    
    // Convert back to FP16 for storage
    cvt.rn.f16.f32 %fh10, %frag_c0;
    cvt.rn.f16.f32 %fh11, %frag_c1;
    cvt.rn.f16.f32 %fh12, %frag_c2;
    cvt.rn.f16.f32 %fh13, %frag_c3;
    cvt.rn.f16.f32 %fh14, %frag_c4;
    cvt.rn.f16.f32 %fh15, %frag_c5;
    cvt.rn.f16.f32 %fh16, %frag_c6;
    cvt.rn.f16.f32 %fh17, %frag_c7;
    
    // Store result using wmma.store
    // Calculate C address: C + row * ldc + col
    mul.lo.u32 %r70, %r20, %r6;
    add.u32 %r70, %r70, %r21;
    shl.b32 %r70, %r70, 1;
    cvt.u64.u32 %rd30, %r70;
    add.u64 %rd30, %rd3, %rd30;
    
    // Store fragment to global memory
    wmma.store.d.sync.aligned.row.m16n16k16.global.f16 [%rd30], {%fh10, %fh11, %fh12, %fh13, %fh14, %fh15, %fh16, %fh17}, %r6;
    
EXIT:
    ret;
}

// INT8 tensor core kernel for ultra-fast inference
.visible .entry gemm_int8_tensor_core(
    .param .u64 param_a,          // Matrix A (INT8)
    .param .u64 param_b,          // Matrix B (INT8)
    .param .u64 param_c,          // Matrix C (INT32)
    .param .u32 param_m,
    .param .u32 param_n,
    .param .u32 param_k,
    .param .u32 param_lda,
    .param .u32 param_ldb,
    .param .u32 param_ldc
)
{
    // INT8 tensor core implementation
    // Achieves ~4x higher throughput than FP16
    ret;
}

// TF32 tensor core kernel for training
.visible .entry gemm_tf32_tensor_core(
    .param .u64 param_a,          // Matrix A (TF32)
    .param .u64 param_b,          // Matrix B (TF32)
    .param .u64 param_c,          // Matrix C (FP32)
    .param .u32 param_m,
    .param .u32 param_n,
    .param .u32 param_k,
    .param .u32 param_lda,
    .param .u32 param_ldb,
    .param .u32 param_ldc
)
{
    // TF32 tensor core implementation
    // Balances precision and performance for training
    ret;
}