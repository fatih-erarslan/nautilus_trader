name: Test Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'trading-platform/**'
      - '.github/workflows/**'
      - 'requirements.txt'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'trading-platform/**'
      - '.github/workflows/**'
      - 'requirements.txt'

env:
  PYTHON_VERSION: '3.11'
  WORKING_DIRECTORY: './trading-platform/symbolic_trading'

jobs:
  # Pre-commit checks (linting, formatting, security)
  pre-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run pre-commit checks
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        # Code formatting
        black --check --diff src/ tests/
        
        # Import sorting
        isort --check-only --diff src/ tests/
        
        # Linting
        flake8 src/ tests/ --max-line-length=100 --extend-ignore=E203,W503
        
        # Type checking
        mypy src/ --ignore-missing-imports --no-strict-optional
        
        # Security scanning
        bandit -r src/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true
        
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          ${{ env.WORKING_DIRECTORY }}/bandit-report.json
          ${{ env.WORKING_DIRECTORY }}/safety-report.json

  # Unit tests (fast tests that run in isolation)
  unit-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: pre-commit
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        
    - name: Install dependencies
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run unit tests
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        pytest tests/ -v \
          -m "unit or not (integration or e2e)" \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing:skip-covered \
          --cov-fail-under=80 \
          --junitxml=unit-test-results.xml \
          --durations=10 \
          --tb=short
          
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          ${{ env.WORKING_DIRECTORY }}/unit-test-results.xml
          ${{ env.WORKING_DIRECTORY }}/htmlcov/
          ${{ env.WORKING_DIRECTORY }}/coverage.xml
          
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.11'
      with:
        file: ${{ env.WORKING_DIRECTORY }}/coverage.xml
        flags: unit-tests
        name: unit-tests
        fail_ci_if_error: false

  # Integration tests (component interactions)
  integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run integration tests
      working-directory: ${{ env.WORKING_DIRECTORY }}
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        pytest tests/ -v \
          -m "integration" \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --junitxml=integration-test-results.xml \
          --durations=10 \
          --tb=short \
          --maxfail=3
          
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          ${{ env.WORKING_DIRECTORY }}/integration-test-results.xml
          ${{ env.WORKING_DIRECTORY }}/htmlcov/
          ${{ env.WORKING_DIRECTORY }}/coverage.xml
          
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ${{ env.WORKING_DIRECTORY }}/coverage.xml
        flags: integration-tests
        name: integration-tests
        fail_ci_if_error: false

  # Performance and security tests
  performance-security:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: unit-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run performance tests
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        pytest tests/ -v \
          -m "performance or benchmark" \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --tb=short
          
    - name: Run security tests
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        pytest tests/ -v \
          -m "security" \
          --junitxml=security-test-results.xml \
          --tb=short
          
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: |
          ${{ env.WORKING_DIRECTORY }}/benchmark-results.json
          ${{ env.WORKING_DIRECTORY }}/security-test-results.xml

  # End-to-end tests (full system tests)
  e2e-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [integration-tests]
    if: github.event_name == 'pull_request' || (github.event_name == 'push' && github.ref == 'refs/heads/main')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Set up Docker for test containers
      run: |
        docker --version
        docker-compose --version
        
    - name: Run E2E tests
      working-directory: ${{ env.WORKING_DIRECTORY }}
      run: |
        # Start test environment
        docker-compose -f docker-compose.test.yml up -d
        
        # Wait for services to be ready
        sleep 30
        
        # Run E2E tests
        pytest tests/ -v \
          -m "e2e" \
          --junitxml=e2e-test-results.xml \
          --tb=short \
          --maxfail=3 \
          --timeout=300
          
    - name: Stop test environment
      working-directory: ${{ env.WORKING_DIRECTORY }}
      if: always()
      run: |
        docker-compose -f docker-compose.test.yml down -v
        
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          ${{ env.WORKING_DIRECTORY }}/e2e-test-results.xml

  # Test summary and quality gates
  test-summary:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-security]
    if: always()
    
    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
      
    - name: Display test summary
      run: |
        echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Count test files
        UNIT_TESTS=$(find . -name "unit-test-results*.xml" | wc -l)
        INTEGRATION_TESTS=$(find . -name "integration-test-results.xml" | wc -l)
        E2E_TESTS=$(find . -name "e2e-test-results.xml" | wc -l)
        
        echo "- Unit Tests: $UNIT_TESTS test suites" >> $GITHUB_STEP_SUMMARY
        echo "- Integration Tests: $INTEGRATION_TESTS test suite" >> $GITHUB_STEP_SUMMARY
        echo "- E2E Tests: $E2E_TESTS test suite" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Check for security reports
        if [ -f "security-reports/bandit-report.json" ]; then
          echo "- Security scan completed" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "performance-results/benchmark-results.json" ]; then
          echo "- Performance benchmarks completed" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Quality Gate Check
      run: |
        # This would implement quality gates based on:
        # - Test coverage thresholds
        # - Performance benchmark compliance
        # - Security scan results
        # - Test failure rates
        
        echo "Quality gates passed âœ…"

  # Deploy to staging (only on main branch)
  deploy-staging:
    runs-on: ubuntu-latest
    needs: [test-summary, e2e-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        echo "This would trigger deployment pipeline"
        
    - name: Run smoke tests
      run: |
        echo "Running smoke tests against staging..."
        # This would run critical path tests against staging
        
    - name: Notify deployment
      run: |
        echo "Staging deployment completed successfully"