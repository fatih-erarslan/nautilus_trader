//! # Dual GPU pBit Solver with Real GPU Acceleration
//!
//! Demonstrates true GPU-accelerated pBit dynamics using WGSL compute shaders
//! on dual AMD GPUs (RX 6800 XT + RX 5500 XT).
//!
//! ## What This Demo Does
//!
//! 1. Initializes both GPUs via wgpu
//! 2. Creates pBit buffers on each GPU
//! 3. Compiles and runs WGSL Metropolis kernel
//! 4. Partitions workload by VRAM capacity (60/40 split)
//! 5. Runs annealing with async boundary exchange
//!
//! Run with:
//! ```bash
//! cargo run --example dual_gpu_pbit --release -p hyperphysics-gpu-unified
//! ```

use hyperphysics_gpu_unified::prelude::*;
use std::sync::Arc;
use std::time::{Duration, Instant};
use wgpu::util::DeviceExt;
use rand::Rng;

/// pBit Metropolis compute shader (embedded)
const PBIT_SHADER: &str = include_str!("../src/kernels/shaders/pbit_metropolis.wgsl");

/// GPU-accelerated pBit partition
struct GpuPBitPartition {
    /// Device for this partition
    device: Arc<wgpu::Device>,
    /// Queue for this partition
    queue: Arc<wgpu::Queue>,
    /// GPU name
    name: String,
    /// Number of pBits
    num_pbits: usize,
    /// Packed state buffer (u32 words)
    state_buffer: wgpu::Buffer,
    /// Bias buffer
    bias_buffer: wgpu::Buffer,
    /// CSR row pointers
    row_ptr_buffer: wgpu::Buffer,
    /// CSR entries
    entries_buffer: wgpu::Buffer,
    /// Params buffer
    params_buffer: wgpu::Buffer,
    /// Metropolis pipeline
    metropolis_pipeline: wgpu::ComputePipeline,
    /// Bind group
    bind_group: wgpu::BindGroup,
    /// Workgroups needed
    num_workgroups: u32,
}

impl GpuPBitPartition {
    fn new(
        device: Arc<wgpu::Device>,
        queue: Arc<wgpu::Queue>,
        name: String,
        num_pbits: usize,
        edges: &[(usize, usize, f32)],
        seed: u32,
    ) -> Self {
        let num_words = (num_pbits + 31) / 32;

        // Initialize random states
        let mut rng = rand::thread_rng();
        let initial_states: Vec<u32> = (0..num_words).map(|_| rng.gen()).collect();

        // Create state buffer
        let state_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("pBit States"),
            contents: bytemuck::cast_slice(&initial_states),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_SRC | wgpu::BufferUsages::COPY_DST,
        });

        // Create bias buffer (all zeros)
        let biases = vec![0.0f32; num_pbits];
        let bias_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("pBit Biases"),
            contents: bytemuck::cast_slice(&biases),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Build CSR from edges
        let (row_ptr, entries) = Self::build_csr(num_pbits, edges);

        let row_ptr_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("CSR Row Pointers"),
            contents: bytemuck::cast_slice(&row_ptr),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let entries_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("CSR Entries"),
            contents: bytemuck::cast_slice(&entries),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Params: [num_pbits, phase, beta, seed]
        let params = [num_pbits as u32, 0u32, 1.0f32.to_bits(), seed];
        let params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("Metropolis Params"),
            contents: bytemuck::cast_slice(&params),
            usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
        });

        // Compile shader
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("pBit Metropolis Shader"),
            source: wgpu::ShaderSource::Wgsl(PBIT_SHADER.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("pBit Bind Group Layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("pBit Pipeline Layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let metropolis_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("Metropolis Pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: "metropolis_kernel",
            compilation_options: Default::default(),
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("pBit Bind Group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry { binding: 0, resource: state_buffer.as_entire_binding() },
                wgpu::BindGroupEntry { binding: 1, resource: bias_buffer.as_entire_binding() },
                wgpu::BindGroupEntry { binding: 2, resource: row_ptr_buffer.as_entire_binding() },
                wgpu::BindGroupEntry { binding: 3, resource: entries_buffer.as_entire_binding() },
                wgpu::BindGroupEntry { binding: 4, resource: params_buffer.as_entire_binding() },
            ],
        });

        // Calculate workgroups (256 threads per workgroup, 2 pBits per thread due to checkerboard)
        let num_workgroups = ((num_pbits / 2 + 255) / 256) as u32;

        Self {
            device,
            queue,
            name,
            num_pbits,
            state_buffer,
            bias_buffer,
            row_ptr_buffer,
            entries_buffer,
            params_buffer,
            metropolis_pipeline,
            bind_group,
            num_workgroups,
        }
    }

    fn build_csr(num_pbits: usize, edges: &[(usize, usize, f32)]) -> (Vec<u32>, Vec<[u32; 2]>) {
        // Build adjacency list
        let mut adj: Vec<Vec<(u32, f32)>> = vec![Vec::new(); num_pbits];
        for &(i, j, w) in edges {
            if i < num_pbits && j < num_pbits {
                adj[i].push((j as u32, w));
                adj[j].push((i as u32, w));
            }
        }

        // Convert to CSR
        let mut row_ptr = vec![0u32; num_pbits + 1];
        let mut entries = Vec::new();

        for i in 0..num_pbits {
            row_ptr[i] = entries.len() as u32;
            for &(target, strength) in &adj[i] {
                entries.push([target, strength.to_bits()]);
            }
        }
        row_ptr[num_pbits] = entries.len() as u32;

        (row_ptr, entries)
    }

    fn update_temperature(&self, beta: f32, phase: u32, seed: u32) {
        let params = [self.num_pbits as u32, phase, beta.to_bits(), seed];
        self.queue.write_buffer(&self.params_buffer, 0, bytemuck::cast_slice(&params));
    }

    fn dispatch_sweep(&self, phase: u32) {
        let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("Metropolis Sweep"),
        });

        {
            let mut pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("Metropolis Pass"),
                timestamp_writes: None,
            });
            pass.set_pipeline(&self.metropolis_pipeline);
            pass.set_bind_group(0, &self.bind_group, &[]);
            pass.dispatch_workgroups(self.num_workgroups, 1, 1);
        }

        self.queue.submit(std::iter::once(encoder.finish()));
    }

    fn full_sweep(&self, beta: f32, seed: u32) {
        // Phase 0: Red (even indices)
        self.update_temperature(beta, 0, seed);
        self.dispatch_sweep(0);

        // Phase 1: Black (odd indices)
        self.update_temperature(beta, 1, seed.wrapping_add(1));
        self.dispatch_sweep(1);
    }

    fn synchronize(&self) {
        self.device.poll(wgpu::Maintain::Wait);
    }
}

/// Dual GPU pBit system
struct DualGpuPBitSystem {
    orchestrator: GpuOrchestrator,
    primary_partition: Option<GpuPBitPartition>,
    secondary_partition: Option<GpuPBitPartition>,
    total_pbits: usize,
}

impl DualGpuPBitSystem {
    fn new(total_pbits: usize, avg_degree: usize) -> GpuResult<Self> {
        // Initialize orchestrator
        let orchestrator = GpuOrchestrator::new()?;
        
        let primary_specs = orchestrator.primary_specs();
        let secondary_specs = orchestrator.secondary_specs();

        println!("\n   GPU Configuration:");
        println!("      Primary:   {}", primary_specs.name);
        if let Some(specs) = secondary_specs {
            println!("      Secondary: {}", specs.name);
        } else {
            println!("      Secondary: None (single GPU mode)");
        }

        // Generate random edges
        let mut rng = rand::thread_rng();
        let target_edges = total_pbits * avg_degree / 2;
        let mut edges = Vec::with_capacity(target_edges);
        
        for _ in 0..target_edges {
            let i = rng.gen_range(0..total_pbits);
            let j = rng.gen_range(0..total_pbits);
            if i != j {
                // Antiferromagnetic for MAX-CUT
                edges.push((i, j, -rng.gen::<f32>()));
            }
        }

        // Partition: 60% primary, 40% secondary (based on VRAM)
        let primary_size = if orchestrator.has_dual_gpu() {
            (total_pbits * 60) / 100
        } else {
            total_pbits
        };
        let secondary_size = total_pbits - primary_size;

        // Split edges between partitions
        let (primary_edges, _secondary_edges): (Vec<_>, Vec<_>) = edges
            .iter()
            .partition(|&&(i, j, _)| i < primary_size && j < primary_size);

        // Get primary device and queue
        let (primary_device, primary_queue) = orchestrator.primary();
        
        // Create primary partition
        let primary_partition = Some(GpuPBitPartition::new(
            primary_device.clone(),
            primary_queue.clone(),
            primary_specs.name.clone(),
            primary_size,
            &primary_edges,
            42,
        ));

        // Create secondary partition if available
        let secondary_partition = if let Some((sec_device, sec_queue)) = orchestrator.secondary() {
            if secondary_size > 0 {
                // Remap indices for secondary partition
                let secondary_edges: Vec<_> = edges
                    .iter()
                    .filter(|&&(i, j, _)| i >= primary_size && j >= primary_size)
                    .map(|&(i, j, w)| (i - primary_size, j - primary_size, w))
                    .collect();

                Some(GpuPBitPartition::new(
                    sec_device.clone(),
                    sec_queue.clone(),
                    secondary_specs.map(|s| s.name.clone()).unwrap_or_default(),
                    secondary_size,
                    &secondary_edges,
                    43,
                ))
            } else {
                None
            }
        } else {
            None
        };

        Ok(Self {
            orchestrator,
            primary_partition,
            secondary_partition,
            total_pbits,
        })
    }

    fn sweep(&self, temperature: f64, seed: u32) {
        let beta = (1.0 / temperature) as f32;

        // Dispatch sweeps on both GPUs in parallel
        if let Some(ref primary) = self.primary_partition {
            primary.full_sweep(beta, seed);
        }

        if let Some(ref secondary) = self.secondary_partition {
            secondary.full_sweep(beta, seed.wrapping_add(1000));
        }

        // Synchronize both GPUs
        if let Some(ref primary) = self.primary_partition {
            primary.synchronize();
        }
        if let Some(ref secondary) = self.secondary_partition {
            secondary.synchronize();
        }
    }

    fn run_annealing(&self, t_start: f64, t_end: f64, steps: usize, sweeps_per_temp: usize) -> Duration {
        let start = Instant::now();

        let ratio = (t_end / t_start).powf(1.0 / (steps - 1) as f64);
        
        for step in 0..steps {
            let temp = t_start * ratio.powi(step as i32);
            let seed = (step * 12345) as u32;

            for _ in 0..sweeps_per_temp {
                self.sweep(temp, seed);
            }

            // Progress
            if step % (steps / 10).max(1) == 0 {
                print!("\r      Progress: {:>3}% | T={:.4}", (step + 1) * 100 / steps, temp);
                std::io::Write::flush(&mut std::io::stdout()).unwrap();
            }
        }
        println!();

        start.elapsed()
    }
}

fn main() -> GpuResult<()> {
    println!("\n");
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—               â•‘");
    println!("â•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘               â•‘");
    println!("â•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘               â•‘");
    println!("â•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘               â•‘");
    println!("â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•               â•‘");
    println!("â•‘    â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•     â•šâ•â•â•â•â•â• â•šâ•â•      â•šâ•â•â•â•â•â•               â•‘");
    println!("â•‘                                                                                   â•‘");
    println!("â•‘           REAL GPU-ACCELERATED pBit Dynamics on Dual AMD Radeon GPUs             â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // Test configurations
    let test_cases = vec![
        (10_000, 20, "10K"),
        (100_000, 15, "100K"),
        (1_000_000, 10, "1M"),
    ];

    for (num_pbits, avg_degree, label) in test_cases {
        println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        println!("â•‘  {} pBits - GPU Accelerated Simulated Annealing", label);
        println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

        // Create system
        print!("   Creating dual GPU system... ");
        std::io::Write::flush(&mut std::io::stdout()).unwrap();
        let start = Instant::now();
        
        let system = match DualGpuPBitSystem::new(num_pbits, avg_degree) {
            Ok(s) => s,
            Err(e) => {
                println!("Failed: {:?}", e);
                continue;
            }
        };
        println!("done in {:.1}ms", start.elapsed().as_millis());

        // Partition info
        if let Some(ref p) = system.primary_partition {
            println!("      Primary:   {} pBits on {}", p.num_pbits, p.name);
        }
        if let Some(ref p) = system.secondary_partition {
            println!("      Secondary: {} pBits on {}", p.num_pbits, p.name);
        }

        // Run annealing
        println!("\n   ğŸ”¥ Running simulated annealing...");
        let annealing_steps = 100;
        let sweeps_per_temp = 10;
        
        let elapsed = system.run_annealing(5.0, 0.01, annealing_steps, sweeps_per_temp);

        let total_sweeps = annealing_steps * sweeps_per_temp;
        let total_spin_updates = total_sweeps * num_pbits;
        let throughput = total_spin_updates as f64 / elapsed.as_secs_f64() / 1e6;

        println!("\n   ğŸ“ˆ Results:");
        println!("      Total time:     {:.2}s", elapsed.as_secs_f64());
        println!("      Total sweeps:   {}", total_sweeps);
        println!("      Throughput:     {:.1}M spin-updates/sec", throughput);
        println!("      Time per sweep: {:.2}ms", elapsed.as_millis() as f64 / total_sweeps as f64);
    }

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                           GPU BENCHMARK COMPLETE                                   â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    Ok(())
}
