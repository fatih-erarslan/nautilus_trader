//! Biological memory system with short-term and long-term consolidation patterns
//!\n//! Implements biologically-inspired memory patterns with:\n//! - Short-term working memory with limited capacity\n//! - Long-term memory consolidation with forgetting curves\n//! - Episodic memory for significant events\n//! - Attention mechanisms for selective memory processing\n//! - SIMD-optimized pattern matching and similarity calculations\n\nuse crate::{QBMIAError, QBMIAResult, types::*, hardware_optimizer::QBMIAHardwareOptimizer};\nuse ndarray::{Array1, Array2};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, VecDeque};\nuse tokio::time::{Duration, Instant};\nuse tracing::{debug, info, warn, error};\nuse rand::prelude::*;\nuse rand_distr::Normal;\nuse rayon::prelude::*;\nuse crossbeam::channel;\nuse dashmap::DashMap;\nuse std::sync::{Arc, RwLock};\n\n/// Memory experience structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MemoryExperience {\n    pub features: Array1<f64>,\n    pub timestamp: chrono::DateTime<chrono::Utc>,\n    pub importance: f64,\n    pub raw_data: serde_json::Value,\n    pub experience_type: ExperienceType,\n    pub consolidation_count: usize,\n}\n\n/// Types of experiences\n#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]\npub enum ExperienceType {\n    MarketAnalysis,\n    DecisionOutcome,\n    ManipulationDetected,\n    VolatilityEvent,\n    SignificantTrade,\n    SystemEvent,\n}\n\n/// Memory consolidation parameters\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ConsolidationParams {\n    pub consolidation_rate: f64,\n    pub forgetting_factor: f64,\n    pub importance_threshold: f64,\n    pub consolidation_interval: Duration,\n    pub max_consolidations: usize,\n}\n\nimpl Default for ConsolidationParams {\n    fn default() -> Self {\n        Self {\n            consolidation_rate: 0.1,\n            forgetting_factor: 0.99,\n            importance_threshold: 0.5,\n            consolidation_interval: Duration::from_secs(60),\n            max_consolidations: 10,\n        }\n    }\n}\n\n/// Attention mechanism parameters\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AttentionParams {\n    pub weights: Array1<f64>,\n    pub focus_areas: Vec<String>,\n    pub attention_decay: f64,\n    pub novelty_boost: f64,\n}\n\nimpl Default for AttentionParams {\n    fn default() -> Self {\n        let weights = Array1::from_elem(128, 1.0 / 128.0);\n        Self {\n            weights,\n            focus_areas: Vec::new(),\n            attention_decay: 0.95,\n            novelty_boost: 1.5,\n        }\n    }\n}\n\n/// Memory statistics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MemoryStats {\n    pub total_stored: usize,\n    pub successful_recalls: usize,\n    pub failed_recalls: usize,\n    pub consolidations: usize,\n    pub average_importance: f64,\n    pub memory_efficiency: f64,\n    pub consolidation_rate: f64,\n}\n\nimpl Default for MemoryStats {\n    fn default() -> Self {\n        Self {\n            total_stored: 0,\n            successful_recalls: 0,\n            failed_recalls: 0,\n            consolidations: 0,\n            average_importance: 0.0,\n            memory_efficiency: 0.0,\n            consolidation_rate: 0.0,\n        }\n    }\n}\n\n/// Biological memory system\npub struct BiologicalMemory {\n    // Configuration\n    capacity: usize,\n    feature_size: usize,\n    consolidation_params: ConsolidationParams,\n    attention_params: AttentionParams,\n    \n    // Memory stores\n    short_term_memory: Arc<RwLock<VecDeque<MemoryExperience>>>,\n    long_term_memory: Arc<DashMap<String, MemoryExperience>>,\n    episodic_memory: Arc<RwLock<VecDeque<MemoryExperience>>>,\n    \n    // Memory indices for fast retrieval\n    feature_index: Arc<DashMap<String, Vec<String>>>, // Feature hash -> Memory IDs\n    temporal_index: Arc<DashMap<String, Vec<String>>>, // Time bucket -> Memory IDs\n    importance_index: Arc<DashMap<String, Vec<String>>>, // Importance level -> Memory IDs\n    \n    // Consolidation state\n    consolidation_buffer: Arc<RwLock<Vec<MemoryExperience>>>,\n    last_consolidation: Arc<RwLock<Instant>>,\n    \n    // Statistics and monitoring\n    memory_stats: Arc<RwLock<MemoryStats>>,\n    recall_threshold: f64,\n    \n    // Hardware optimization\n    hw_optimizer: Option<Arc<QBMIAHardwareOptimizer>>,\n}\n\nimpl BiologicalMemory {\n    /// Create new biological memory system\n    pub fn new(capacity: usize, hw_optimizer: Option<Arc<QBMIAHardwareOptimizer>>) -> Self {\n        info!(\"Initializing biological memory with capacity {}\", capacity);\n        \n        Self {\n            capacity,\n            feature_size: 128, // Default feature vector size\n            consolidation_params: ConsolidationParams::default(),\n            attention_params: AttentionParams::default(),\n            \n            short_term_memory: Arc::new(RwLock::new(VecDeque::with_capacity(100))),\n            long_term_memory: Arc::new(DashMap::with_capacity(capacity)),\n            episodic_memory: Arc::new(RwLock::new(VecDeque::with_capacity(1000))),\n            \n            feature_index: Arc::new(DashMap::new()),\n            temporal_index: Arc::new(DashMap::new()),\n            importance_index: Arc::new(DashMap::new()),\n            \n            consolidation_buffer: Arc::new(RwLock::new(Vec::new())),\n            last_consolidation: Arc::new(RwLock::new(Instant::now())),\n            \n            memory_stats: Arc::new(RwLock::new(MemoryStats::default())),\n            recall_threshold: 0.7,\n            \n            hw_optimizer,\n        }\n    }\n    \n    /// Store new experience in memory\n    pub async fn store_experience(&self, experience_data: serde_json::Value) -> QBMIAResult<()> {\n        let features = self.extract_features(&experience_data)?;\n        let importance = self.calculate_importance(&experience_data)?;\n        let experience_type = self.classify_experience(&experience_data);\n        \n        let experience = MemoryExperience {\n            features,\n            timestamp: chrono::Utc::now(),\n            importance,\n            raw_data: experience_data.clone(),\n            experience_type,\n            consolidation_count: 0,\n        };\n        \n        // Store in short-term memory\n        {\n            let mut stm = self.short_term_memory.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire short-term memory lock: {}\", e))\n            })?;\n            \n            stm.push_back(experience.clone());\n            \n            // Maintain capacity\n            if stm.len() > 100 {\n                stm.pop_front();\n            }\n        }\n        \n        // Store significant episodes\n        if self.is_significant_episode(&experience) {\n            let mut episodic = self.episodic_memory.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire episodic memory lock: {}\", e))\n            })?;\n            \n            episodic.push_back(experience.clone());\n            \n            if episodic.len() > 1000 {\n                episodic.pop_front();\n            }\n        }\n        \n        // Update indices\n        self.update_indices(&experience).await?;\n        \n        // Update statistics\n        {\n            let mut stats = self.memory_stats.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire stats lock: {}\", e))\n            })?;\n            stats.total_stored += 1;\n            stats.average_importance = (stats.average_importance * (stats.total_stored - 1) as f64 + importance) / stats.total_stored as f64;\n        }\n        \n        // Check if consolidation is needed\n        self.check_consolidation_trigger().await?;\n        \n        Ok(())\n    }\n    \n    /// Extract feature vector from experience data\n    fn extract_features(&self, experience_data: &serde_json::Value) -> QBMIAResult<Array1<f64>> {\n        let mut features = Array1::zeros(self.feature_size);\n        \n        // Extract market features\n        if let Some(market_snapshot) = experience_data.get(\"market_snapshot\") {\n            if let Some(price) = market_snapshot.get(\"price\").and_then(|v| v.as_f64()) {\n                features[0] = price / 10000.0; // Normalize price\n            }\n            if let Some(volume) = market_snapshot.get(\"volume\").and_then(|v| v.as_f64()) {\n                features[1] = volume / 1e6; // Normalize volume\n            }\n            if let Some(volatility) = market_snapshot.get(\"volatility\").and_then(|v| v.as_f64()) {\n                features[2] = volatility;\n            }\n            if let Some(trend) = market_snapshot.get(\"trend\").and_then(|v| v.as_f64()) {\n                features[3] = trend;\n            }\n        }\n        \n        // Extract decision features\n        if let Some(decision) = experience_data.get(\"integrated_decision\") {\n            if let Some(action) = decision.get(\"action\").and_then(|v| v.as_str()) {\n                let action_idx = match action {\n                    \"buy\" => 0,\n                    \"sell\" => 1,\n                    \"hold\" => 2,\n                    \"wait\" => 3,\n                    _ => 2,\n                };\n                \n                // One-hot encoding for action\n                for i in 0..4 {\n                    features[10 + i] = if i == action_idx { 1.0 } else { 0.0 };\n                }\n            }\n            \n            if let Some(confidence) = decision.get(\"confidence\").and_then(|v| v.as_f64()) {\n                features[14] = confidence;\n            }\n        }\n        \n        // Extract component results\n        if let Some(component_results) = experience_data.get(\"component_results\") {\n            // Quantum Nash features\n            if let Some(qn) = component_results.get(\"quantum_nash\") {\n                if let Some(equilibrium) = qn.get(\"equilibrium\") {\n                    if let Some(score) = equilibrium.get(\"convergence_score\").and_then(|v| v.as_f64()) {\n                        features[20] = score;\n                    }\n                }\n            }\n            \n            // Machiavellian features\n            if let Some(mach) = component_results.get(\"machiavellian\") {\n                if let Some(detected) = mach.get(\"manipulation_detected\") {\n                    if let Some(flag) = detected.get(\"detected\").and_then(|v| v.as_bool()) {\n                        features[30] = if flag { 1.0 } else { 0.0 };\n                    }\n                    if let Some(confidence) = detected.get(\"confidence\").and_then(|v| v.as_f64()) {\n                        features[31] = confidence;\n                    }\n                }\n            }\n            \n            // Additional component features would be extracted here...\n        }\n        \n        // Apply attention weights\n        features = &features * &self.attention_params.weights;\n        \n        // Normalize features\n        let norm = features.mapv(|x| x * x).sum().sqrt();\n        if norm > 1e-8 {\n            features = features / norm;\n        }\n        \n        Ok(features)\n    }\n    \n    /// Calculate importance score for experience\n    fn calculate_importance(&self, experience_data: &serde_json::Value) -> QBMIAResult<f64> {\n        let mut importance = 0.0;\n        \n        // High confidence decisions are important\n        if let Some(decision) = experience_data.get(\"integrated_decision\") {\n            if let Some(confidence) = decision.get(\"confidence\").and_then(|v| v.as_f64()) {\n                importance += confidence * 0.3;\n            }\n        }\n        \n        // Unusual market conditions are important\n        if let Some(snapshot) = experience_data.get(\"market_snapshot\") {\n            if let Some(volatility) = snapshot.get(\"volatility\").and_then(|v| v.as_f64()) {\n                if volatility > 0.03 {\n                    importance += 0.3;\n                }\n            }\n        }\n        \n        // Manipulation detection is important\n        if let Some(results) = experience_data.get(\"component_results\") {\n            if let Some(mach) = results.get(\"machiavellian\") {\n                if let Some(detected) = mach.get(\"manipulation_detected\") {\n                    if let Some(flag) = detected.get(\"detected\").and_then(|v| v.as_bool()) {\n                        if flag {\n                            importance += 0.4;\n                        }\n                    }\n                }\n            }\n        }\n        \n        // Novelty boost for rare events\n        let novelty_factor = self.calculate_novelty_factor(experience_data)?;\n        importance += novelty_factor * self.attention_params.novelty_boost * 0.2;\n        \n        Ok(importance.min(1.0))\n    }\n    \n    /// Calculate novelty factor for experience\n    fn calculate_novelty_factor(&self, _experience_data: &serde_json::Value) -> QBMIAResult<f64> {\n        // Simplified novelty calculation\n        // In practice, this would compare against historical patterns\n        Ok(thread_rng().gen_range(0.0..0.5))\n    }\n    \n    /// Classify experience type\n    fn classify_experience(&self, experience_data: &serde_json::Value) -> ExperienceType {\n        if experience_data.get(\"component_results\").is_some() {\n            if let Some(results) = experience_data.get(\"component_results\") {\n                if results.get(\"machiavellian\").is_some() {\n                    return ExperienceType::ManipulationDetected;\n                }\n            }\n            ExperienceType::MarketAnalysis\n        } else if experience_data.get(\"integrated_decision\").is_some() {\n            ExperienceType::DecisionOutcome\n        } else {\n            ExperienceType::SystemEvent\n        }\n    }\n    \n    /// Check if experience is significant for episodic memory\n    fn is_significant_episode(&self, experience: &MemoryExperience) -> bool {\n        match experience.experience_type {\n            ExperienceType::ManipulationDetected => true,\n            ExperienceType::VolatilityEvent => true,\n            ExperienceType::SignificantTrade => true,\n            _ => experience.importance > 0.7,\n        }\n    }\n    \n    /// Update memory indices\n    async fn update_indices(&self, experience: &MemoryExperience) -> QBMIAResult<()> {\n        let memory_id = self.generate_memory_id(experience);\n        \n        // Feature index\n        let feature_hash = self.hash_features(&experience.features);\n        self.feature_index.entry(feature_hash)\n            .or_insert_with(Vec::new)\n            .push(memory_id.clone());\n        \n        // Temporal index\n        let time_bucket = self.get_time_bucket(&experience.timestamp);\n        self.temporal_index.entry(time_bucket)\n            .or_insert_with(Vec::new)\n            .push(memory_id.clone());\n        \n        // Importance index\n        let importance_level = self.get_importance_level(experience.importance);\n        self.importance_index.entry(importance_level)\n            .or_insert_with(Vec::new)\n            .push(memory_id.clone());\n        \n        Ok(())\n    }\n    \n    /// Generate unique memory ID\n    fn generate_memory_id(&self, experience: &MemoryExperience) -> String {\n        format!(\"{}_{}_{}\", \n            experience.timestamp.timestamp_millis(),\n            experience.experience_type as u8,\n            thread_rng().gen::<u32>()\n        )\n    }\n    \n    /// Hash feature vector for indexing\n    fn hash_features(&self, features: &Array1<f64>) -> String {\n        use std::collections::hash_map::DefaultHasher;\n        use std::hash::{Hash, Hasher};\n        \n        let mut hasher = DefaultHasher::new();\n        \n        // Hash significant features only\n        for (i, &value) in features.iter().enumerate() {\n            if value.abs() > 1e-6 {\n                i.hash(&mut hasher);\n                (value * 1000.0) as i64.hash(&mut hasher);\n            }\n        }\n        \n        format!(\"{:016x}\", hasher.finish())\n    }\n    \n    /// Get time bucket for temporal indexing\n    fn get_time_bucket(&self, timestamp: &chrono::DateTime<chrono::Utc>) -> String {\n        // Group by hour\n        format!(\"{}-{:02}-{:02}T{:02}\", \n            timestamp.year(),\n            timestamp.month(),\n            timestamp.day(),\n            timestamp.hour()\n        )\n    }\n    \n    /// Get importance level for indexing\n    fn get_importance_level(&self, importance: f64) -> String {\n        if importance >= 0.8 {\n            \"critical\".to_string()\n        } else if importance >= 0.6 {\n            \"high\".to_string()\n        } else if importance >= 0.4 {\n            \"medium\".to_string()\n        } else {\n            \"low\".to_string()\n        }\n    }\n    \n    /// Check if consolidation should be triggered\n    async fn check_consolidation_trigger(&self) -> QBMIAResult<()> {\n        let last_consolidation = *self.last_consolidation.read().map_err(|e| {\n            QBMIAError::MemoryError(format!(\"Failed to read consolidation timestamp: {}\", e))\n        })?;\n        \n        if last_consolidation.elapsed() >= self.consolidation_params.consolidation_interval {\n            let stm_size = self.short_term_memory.read().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to read short-term memory: {}\", e))\n            })?.len();\n            \n            if stm_size >= 10 {\n                self.consolidate_memories().await?;\n            }\n        }\n        \n        Ok(())\n    }\n    \n    /// Consolidate memories from short-term to long-term\n    async fn consolidate_memories(&self) -> QBMIAResult<()> {\n        debug!(\"Starting memory consolidation\");\n        \n        // Get experiences from short-term memory\n        let experiences = {\n            let stm = self.short_term_memory.read().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to read short-term memory: {}\", e))\n            })?;\n            \n            stm.iter().rev().take(10).cloned().collect::<Vec<_>>()\n        };\n        \n        if experiences.is_empty() {\n            return Ok(());\n        }\n        \n        // Extract patterns and importance weights\n        let patterns: Vec<Array1<f64>> = experiences.iter().map(|e| e.features.clone()).collect();\n        let importance_weights: Vec<f64> = experiences.iter().map(|e| e.importance).collect();\n        \n        // Calculate consolidated pattern\n        let consolidated_pattern = self.calculate_consolidated_pattern(&patterns, &importance_weights)?;\n        \n        // Create consolidated experience\n        let consolidated_experience = MemoryExperience {\n            features: consolidated_pattern,\n            timestamp: chrono::Utc::now(),\n            importance: importance_weights.iter().sum::<f64>() / importance_weights.len() as f64,\n            raw_data: serde_json::json!({\n                \"type\": \"consolidated\",\n                \"source_count\": experiences.len(),\n                \"consolidation_time\": chrono::Utc::now()\n            }),\n            experience_type: ExperienceType::SystemEvent,\n            consolidation_count: 0,\n        };\n        \n        // Store in long-term memory\n        let memory_id = self.generate_memory_id(&consolidated_experience);\n        self.long_term_memory.insert(memory_id.clone(), consolidated_experience.clone());\n        \n        // Update indices\n        self.update_indices(&consolidated_experience).await?;\n        \n        // Apply biological consolidation with forgetting\n        self.apply_biological_consolidation().await?;\n        \n        // Update statistics\n        {\n            let mut stats = self.memory_stats.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire stats lock: {}\", e))\n            })?;\n            stats.consolidations += 1;\n            stats.consolidation_rate = stats.consolidations as f64 / stats.total_stored as f64;\n        }\n        \n        // Update last consolidation time\n        *self.last_consolidation.write().map_err(|e| {\n            QBMIAError::MemoryError(format!(\"Failed to update consolidation time: {}\", e))\n        })? = Instant::now();\n        \n        info!(\"Memory consolidation completed: {} experiences consolidated\", experiences.len());\n        \n        Ok(())\n    }\n    \n    /// Calculate consolidated pattern from multiple patterns\n    fn calculate_consolidated_pattern(&self, patterns: &[Array1<f64>], weights: &[f64]) -> QBMIAResult<Array1<f64>> {\n        if patterns.is_empty() {\n            return Err(QBMIAError::MemoryError(\"No patterns to consolidate\".to_string()));\n        }\n        \n        let feature_size = patterns[0].len();\n        let mut consolidated = Array1::zeros(feature_size);\n        \n        // Normalize weights\n        let weight_sum: f64 = weights.iter().sum();\n        if weight_sum <= 0.0 {\n            return Err(QBMIAError::MemoryError(\"Invalid weights for consolidation\".to_string()));\n        }\n        \n        // Weighted average\n        for (pattern, &weight) in patterns.iter().zip(weights.iter()) {\n            consolidated = consolidated + pattern * (weight / weight_sum);\n        }\n        \n        Ok(consolidated)\n    }\n    \n    /// Apply biological consolidation with forgetting curves\n    async fn apply_biological_consolidation(&self) -> QBMIAResult<()> {\n        // Apply forgetting curve to long-term memories\n        let forgetting_factor = self.consolidation_params.forgetting_factor;\n        let mut memories_to_remove = Vec::new();\n        \n        for mut entry in self.long_term_memory.iter_mut() {\n            let memory = entry.value_mut();\n            let age_seconds = chrono::Utc::now().signed_duration_since(memory.timestamp).num_seconds() as f64;\n            let age_factor = forgetting_factor.powf(age_seconds / 3600.0); // Hourly decay\n            \n            // Apply forgetting to features\n            memory.features = &memory.features * age_factor;\n            \n            // Remove very old or unimportant memories\n            if age_factor < 0.01 || memory.importance < 0.1 {\n                memories_to_remove.push(entry.key().clone());\n            }\n        }\n        \n        // Remove forgotten memories\n        for memory_id in memories_to_remove {\n            self.long_term_memory.remove(&memory_id);\n        }\n        \n        Ok(())\n    }\n    \n    /// Recall similar experiences from memory\n    pub async fn recall_similar_experiences(&self, query_data: &serde_json::Value, top_k: usize) -> QBMIAResult<Vec<MemoryExperience>> {\n        let query_features = self.extract_features(query_data)?;\n        \n        // Search in long-term memory\n        let mut similarities = Vec::new();\n        \n        for entry in self.long_term_memory.iter() {\n            let memory = entry.value();\n            let similarity = self.calculate_similarity(&query_features, &memory.features)?;\n            \n            if similarity > self.recall_threshold {\n                similarities.push((memory.clone(), similarity));\n            }\n        }\n        \n        // Search in episodic memory\n        {\n            let episodic = self.episodic_memory.read().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to read episodic memory: {}\", e))\n            })?;\n            \n            for memory in episodic.iter().rev().take(100) {\n                let similarity = self.calculate_similarity(&query_features, &memory.features)?;\n                if similarity > self.recall_threshold {\n                    similarities.push((memory.clone(), similarity));\n                }\n            }\n        }\n        \n        // Sort by similarity and return top-k\n        similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n        \n        let recalled: Vec<MemoryExperience> = similarities\n            .into_iter()\n            .take(top_k)\n            .map(|(memory, _)| memory)\n            .collect();\n        \n        // Update statistics\n        {\n            let mut stats = self.memory_stats.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire stats lock: {}\", e))\n            })?;\n            \n            if recalled.is_empty() {\n                stats.failed_recalls += 1;\n            } else {\n                stats.successful_recalls += 1;\n            }\n            \n            let total_recalls = stats.successful_recalls + stats.failed_recalls;\n            stats.memory_efficiency = stats.successful_recalls as f64 / total_recalls as f64;\n        }\n        \n        Ok(recalled)\n    }\n    \n    /// Calculate similarity between two feature vectors\n    fn calculate_similarity(&self, features1: &Array1<f64>, features2: &Array1<f64>) -> QBMIAResult<f64> {\n        if let Some(ref hw_optimizer) = self.hw_optimizer {\n            // Use hardware-optimized similarity calculation\n            Ok(hw_optimizer.calculate_similarity(features1, features2))\n        } else {\n            // Standard cosine similarity\n            let dot_product = features1.dot(features2);\n            let norm1 = features1.mapv(|x| x * x).sum().sqrt();\n            let norm2 = features2.mapv(|x| x * x).sum().sqrt();\n            \n            if norm1 * norm2 == 0.0 {\n                Ok(0.0)\n            } else {\n                Ok((dot_product / (norm1 * norm2)).max(0.0).min(1.0))\n            }\n        }\n    }\n    \n    /// Get recent patterns for analysis\n    pub async fn get_recent_patterns(&self, window: usize) -> QBMIAResult<Vec<Array1<f64>>> {\n        let mut patterns = Vec::new();\n        \n        // From short-term memory\n        {\n            let stm = self.short_term_memory.read().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to read short-term memory: {}\", e))\n            })?;\n            \n            for memory in stm.iter().rev().take(window) {\n                patterns.push(memory.features.clone());\n            }\n        }\n        \n        // From recent long-term consolidations\n        let recent_ltm: Vec<MemoryExperience> = self.long_term_memory\n            .iter()\n            .map(|entry| entry.value().clone())\n            .collect::<Vec<_>>()\n            .into_iter()\n            .filter(|m| chrono::Utc::now().signed_duration_since(m.timestamp).num_hours() < 24)\n            .collect();\n        \n        for memory in recent_ltm.into_iter().take(window) {\n            patterns.push(memory.features);\n        }\n        \n        Ok(patterns)\n    }\n    \n    /// Apply attention mechanism to focus on specific areas\n    pub async fn apply_attention(&mut self, focus_areas: &[String]) -> QBMIAResult<()> {\n        info!(\"Applying attention to areas: {:?}\", focus_areas);\n        \n        // Reset attention weights\n        self.attention_params.weights.fill(1.0 / self.feature_size as f64);\n        \n        // Increase weights for focus areas\n        for area in focus_areas {\n            match area.as_str() {\n                \"volatility\" => {\n                    self.attention_params.weights[2] *= 2.0; // Volatility feature\n                    for i in 40..50 {\n                        self.attention_params.weights[i] *= 1.5; // Volatility-related features\n                    }\n                }\n                \"manipulation\" => {\n                    for i in 30..40 {\n                        self.attention_params.weights[i] *= 2.0; // Manipulation features\n                    }\n                }\n                \"quantum\" => {\n                    for i in 20..30 {\n                        self.attention_params.weights[i] *= 1.5; // Quantum features\n                    }\n                }\n                \"decisions\" => {\n                    for i in 10..20 {\n                        self.attention_params.weights[i] *= 1.8; // Decision features\n                    }\n                }\n                _ => {}\n            }\n        }\n        \n        // Normalize weights\n        let weight_sum = self.attention_params.weights.sum();\n        if weight_sum > 0.0 {\n            self.attention_params.weights = &self.attention_params.weights / weight_sum;\n        }\n        \n        self.attention_params.focus_areas = focus_areas.to_vec();\n        \n        Ok(())\n    }\n    \n    /// Get memory usage statistics\n    pub fn get_usage_stats(&self) -> MemoryUsage {\n        let stm_size = self.short_term_memory.read().map_or(0, |stm| stm.len());\n        let ltm_size = self.long_term_memory.len();\n        let episodic_size = self.episodic_memory.read().map_or(0, |em| em.len());\n        \n        let feature_size_bytes = self.feature_size * 8; // 8 bytes per f64\n        let stm_memory_mb = (stm_size * feature_size_bytes) as f64 / (1024.0 * 1024.0);\n        let ltm_memory_mb = (ltm_size * feature_size_bytes) as f64 / (1024.0 * 1024.0);\n        let total_memory_mb = stm_memory_mb + ltm_memory_mb;\n        \n        MemoryUsage {\n            short_term_memory_mb: stm_memory_mb,\n            long_term_memory_mb: ltm_memory_mb,\n            total_memory_mb,\n            capacity_percentage: (ltm_size as f64 / self.capacity as f64) * 100.0,\n            consolidation_rate: self.consolidation_params.consolidation_rate,\n        }\n    }\n    \n    /// Get memory statistics\n    pub fn get_memory_stats(&self) -> MemoryStats {\n        self.memory_stats.read().map_or(MemoryStats::default(), |stats| stats.clone())\n    }\n    \n    /// Serialize memory state\n    pub async fn serialize(&self) -> QBMIAResult<serde_json::Value> {\n        let stm: Vec<MemoryExperience> = self.short_term_memory.read()\n            .map_err(|e| QBMIAError::MemoryError(format!(\"Failed to read short-term memory: {}\", e)))?\n            .iter().cloned().collect();\n        \n        let ltm: Vec<MemoryExperience> = self.long_term_memory.iter()\n            .map(|entry| entry.value().clone())\n            .collect();\n        \n        let episodic: Vec<MemoryExperience> = self.episodic_memory.read()\n            .map_err(|e| QBMIAError::MemoryError(format!(\"Failed to read episodic memory: {}\", e)))?\n            .iter().cloned().collect();\n        \n        let serialized = serde_json::json!({\n            \"capacity\": self.capacity,\n            \"feature_size\": self.feature_size,\n            \"consolidation_params\": self.consolidation_params,\n            \"attention_params\": {\n                \"weights\": self.attention_params.weights.to_vec(),\n                \"focus_areas\": self.attention_params.focus_areas,\n                \"attention_decay\": self.attention_params.attention_decay,\n                \"novelty_boost\": self.attention_params.novelty_boost\n            },\n            \"short_term_memory\": stm,\n            \"long_term_memory\": ltm,\n            \"episodic_memory\": episodic,\n            \"memory_stats\": self.get_memory_stats(),\n            \"recall_threshold\": self.recall_threshold\n        });\n        \n        Ok(serialized)\n    }\n    \n    /// Restore memory from serialized state\n    pub async fn restore(&mut self, state: &serde_json::Value) -> QBMIAResult<()> {\n        // Restore configuration\n        if let Some(capacity) = state.get(\"capacity\").and_then(|v| v.as_u64()) {\n            self.capacity = capacity as usize;\n        }\n        \n        if let Some(feature_size) = state.get(\"feature_size\").and_then(|v| v.as_u64()) {\n            self.feature_size = feature_size as usize;\n        }\n        \n        if let Some(params) = state.get(\"consolidation_params\") {\n            self.consolidation_params = serde_json::from_value(params.clone())?;\n        }\n        \n        // Restore attention parameters\n        if let Some(attention) = state.get(\"attention_params\") {\n            if let Some(weights) = attention.get(\"weights\").and_then(|v| v.as_array()) {\n                let weight_vec: Result<Vec<f64>, _> = weights.iter().map(|v| v.as_f64().ok_or(\"Invalid weight\")).collect();\n                if let Ok(weights) = weight_vec {\n                    self.attention_params.weights = Array1::from_vec(weights);\n                }\n            }\n            \n            if let Some(focus) = attention.get(\"focus_areas\").and_then(|v| v.as_array()) {\n                self.attention_params.focus_areas = focus.iter()\n                    .filter_map(|v| v.as_str().map(|s| s.to_string()))\n                    .collect();\n            }\n        }\n        \n        // Restore memories\n        if let Some(stm_data) = state.get(\"short_term_memory\") {\n            let stm_experiences: Vec<MemoryExperience> = serde_json::from_value(stm_data.clone())?;\n            let mut stm = self.short_term_memory.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire short-term memory lock: {}\", e))\n            })?;\n            stm.clear();\n            stm.extend(stm_experiences);\n        }\n        \n        if let Some(ltm_data) = state.get(\"long_term_memory\") {\n            let ltm_experiences: Vec<MemoryExperience> = serde_json::from_value(ltm_data.clone())?;\n            self.long_term_memory.clear();\n            for experience in ltm_experiences {\n                let memory_id = self.generate_memory_id(&experience);\n                self.long_term_memory.insert(memory_id, experience);\n            }\n        }\n        \n        if let Some(episodic_data) = state.get(\"episodic_memory\") {\n            let episodic_experiences: Vec<MemoryExperience> = serde_json::from_value(episodic_data.clone())?;\n            let mut episodic = self.episodic_memory.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire episodic memory lock: {}\", e))\n            })?;\n            episodic.clear();\n            episodic.extend(episodic_experiences);\n        }\n        \n        // Restore statistics\n        if let Some(stats_data) = state.get(\"memory_stats\") {\n            let stats: MemoryStats = serde_json::from_value(stats_data.clone())?;\n            *self.memory_stats.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire stats lock: {}\", e))\n            })? = stats;\n        }\n        \n        if let Some(threshold) = state.get(\"recall_threshold\").and_then(|v| v.as_f64()) {\n            self.recall_threshold = threshold;\n        }\n        \n        info!(\"Memory state restored successfully\");\n        Ok(())\n    }\n    \n    /// Cleanup memory resources\n    pub async fn cleanup(&mut self) -> QBMIAResult<()> {\n        // Clear all memory stores\n        {\n            let mut stm = self.short_term_memory.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire short-term memory lock: {}\", e))\n            })?;\n            stm.clear();\n        }\n        \n        self.long_term_memory.clear();\n        \n        {\n            let mut episodic = self.episodic_memory.write().map_err(|e| {\n                QBMIAError::MemoryError(format!(\"Failed to acquire episodic memory lock: {}\", e))\n            })?;\n            episodic.clear();\n        }\n        \n        // Clear indices\n        self.feature_index.clear();\n        self.temporal_index.clear();\n        self.importance_index.clear();\n        \n        // Reset statistics\n        *self.memory_stats.write().map_err(|e| {\n            QBMIAError::MemoryError(format!(\"Failed to acquire stats lock: {}\", e))\n        })? = MemoryStats::default();\n        \n        info!(\"Memory cleanup completed\");\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[tokio::test]\n    async fn test_memory_creation() {\n        let memory = BiologicalMemory::new(1000, None);\n        assert_eq!(memory.capacity, 1000);\n        assert_eq!(memory.feature_size, 128);\n    }\n    \n    #[tokio::test]\n    async fn test_experience_storage() {\n        let memory = BiologicalMemory::new(1000, None);\n        let experience_data = serde_json::json!({\n            \"market_snapshot\": {\n                \"price\": 100.0,\n                \"volume\": 1000.0,\n                \"volatility\": 0.02\n            }\n        });\n        \n        let result = memory.store_experience(experience_data).await;\n        assert!(result.is_ok());\n        \n        let stats = memory.get_memory_stats();\n        assert_eq!(stats.total_stored, 1);\n    }\n    \n    #[tokio::test]\n    async fn test_feature_extraction() {\n        let memory = BiologicalMemory::new(1000, None);\n        let experience_data = serde_json::json!({\n            \"market_snapshot\": {\n                \"price\": 100.0,\n                \"volume\": 1000.0,\n                \"volatility\": 0.02,\n                \"trend\": 0.1\n            },\n            \"integrated_decision\": {\n                \"action\": \"buy\",\n                \"confidence\": 0.8\n            }\n        });\n        \n        let features = memory.extract_features(&experience_data).unwrap();\n        assert_eq!(features.len(), 128);\n        assert_eq!(features[0], 100.0 / 10000.0); // Normalized price\n        assert_eq!(features[10], 1.0); // Buy action one-hot\n    }\n    \n    #[tokio::test]\n    async fn test_similarity_calculation() {\n        let memory = BiologicalMemory::new(1000, None);\n        let features1 = Array1::from(vec![1.0, 0.0, 0.0]);\n        let features2 = Array1::from(vec![1.0, 0.0, 0.0]);\n        \n        let similarity = memory.calculate_similarity(&features1, &features2).unwrap();\n        assert!((similarity - 1.0).abs() < 1e-6);\n    }\n    \n    #[tokio::test]\n    async fn test_memory_recall() {\n        let memory = BiologicalMemory::new(1000, None);\n        \n        // Store some experiences\n        let experience_data = serde_json::json!({\n            \"market_snapshot\": {\n                \"price\": 100.0,\n                \"volume\": 1000.0,\n                \"volatility\": 0.02\n            }\n        });\n        \n        memory.store_experience(experience_data.clone()).await.unwrap();\n        \n        // Recall similar experiences\n        let recalled = memory.recall_similar_experiences(&experience_data, 5).await.unwrap();\n        assert!(!recalled.is_empty());\n    }\n    \n    #[tokio::test]\n    async fn test_attention_mechanism() {\n        let mut memory = BiologicalMemory::new(1000, None);\n        let focus_areas = vec![\"volatility\".to_string(), \"manipulation\".to_string()];\n        \n        let result = memory.apply_attention(&focus_areas).await;\n        assert!(result.is_ok());\n        assert_eq!(memory.attention_params.focus_areas, focus_areas);\n    }\n    \n    #[tokio::test]\n    async fn test_memory_serialization() {\n        let mut memory = BiologicalMemory::new(1000, None);\n        \n        // Store some data\n        let experience_data = serde_json::json!({\n            \"market_snapshot\": {\n                \"price\": 100.0,\n                \"volume\": 1000.0\n            }\n        });\n        \n        memory.store_experience(experience_data).await.unwrap();\n        \n        // Serialize\n        let serialized = memory.serialize().await.unwrap();\n        assert!(serialized.get(\"capacity\").is_some());\n        assert!(serialized.get(\"short_term_memory\").is_some());\n        \n        // Restore\n        let mut new_memory = BiologicalMemory::new(500, None);\n        let result = new_memory.restore(&serialized).await;\n        assert!(result.is_ok());\n        assert_eq!(new_memory.capacity, 1000);\n    }\n}\n\n/// Test memory system functionality\npub fn test_memory_system() -> bool {\n    let rt = tokio::runtime::Runtime::new().unwrap();\n    \n    rt.block_on(async {\n        let memory = BiologicalMemory::new(1000, None);\n        \n        // Test basic functionality\n        let test_data = serde_json::json!({\n            \"market_snapshot\": {\n                \"price\": 100.0,\n                \"volume\": 1000.0,\n                \"volatility\": 0.02\n            }\n        });\n        \n        match memory.store_experience(test_data).await {\n            Ok(_) => {\n                let stats = memory.get_memory_stats();\n                stats.total_stored > 0\n            }\n            Err(_) => false,\n        }\n    })\n}\n\n/// Memory system capabilities\npub fn get_memory_capabilities() -> MemoryCapabilities {\n    MemoryCapabilities {\n        short_term_capacity: 100,\n        long_term_capacity: 10000,\n        episodic_capacity: 1000,\n        feature_dimensions: 128,\n        consolidation_enabled: true,\n        attention_mechanism: true,\n        pattern_matching: true,\n        forgetting_curves: true,\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MemoryCapabilities {\n    pub short_term_capacity: usize,\n    pub long_term_capacity: usize,\n    pub episodic_capacity: usize,\n    pub feature_dimensions: usize,\n    pub consolidation_enabled: bool,\n    pub attention_mechanism: bool,\n    pub pattern_matching: bool,\n    pub forgetting_curves: bool,\n}\n"