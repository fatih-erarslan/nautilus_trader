# High-Frequency Trading Optimization Implementation Guide
## Sub-Millisecond Latency Achievment Plan

---

## Critical Path Optimizations

### 1. Memory Pool Implementation

Create `/src/performance/memory_pool.rs`:

```rust
//! Lock-free memory pool for zero-allocation hot paths

use std::sync::atomic::{AtomicPtr, Ordering};
use std::alloc::{alloc, dealloc, Layout};
use std::ptr::{self, NonNull};

/// High-performance memory pool for trading systems
pub struct HftMemoryPool<T> {
    free_list: AtomicPtr<Node<T>>,
    capacity: usize,
    allocated: std::sync::atomic::AtomicUsize,
}

struct Node<T> {
    next: *mut Node<T>,
    data: T,
}

impl<T> HftMemoryPool<T> {
    pub fn new(capacity: usize) -> Self {
        let pool = Self {
            free_list: AtomicPtr::new(ptr::null_mut()),
            capacity,
            allocated: std::sync::atomic::AtomicUsize::new(0),
        };
        
        // Pre-allocate objects for immediate availability
        for _ in 0..capacity {
            let layout = Layout::new::<Node<T>>();
            unsafe {
                let ptr = alloc(layout) as *mut Node<T>;
                (*ptr).next = pool.free_list.load(Ordering::Relaxed);
                pool.free_list.store(ptr, Ordering::Relaxed);
            }
        }
        
        pool
    }
    
    /// Get object from pool (lock-free, <10ns typical)
    #[inline(always)]
    pub fn acquire(&self) -> Option<Box<T>> {
        loop {
            let head = self.free_list.load(Ordering::Acquire);
            if head.is_null() {
                return None; // Pool exhausted
            }
            
            let next = unsafe { (*head).next };
            
            // Try to update head to next
            match self.free_list.compare_exchange_weak(
                head, next, Ordering::Release, Ordering::Relaxed
            ) {
                Ok(_) => {
                    self.allocated.fetch_add(1, Ordering::Relaxed);
                    unsafe {
                        let data = ptr::read(&(*head).data);
                        dealloc(head as *mut u8, Layout::new::<Node<T>>());
                        return Some(Box::new(data));
                    }
                }
                Err(_) => continue, // Retry on contention
            }
        }
    }
    
    /// Return object to pool (lock-free, <5ns typical)
    #[inline(always)]
    pub fn release(&self, _obj: Box<T>) {
        let layout = Layout::new::<Node<T>>();
        unsafe {
            let ptr = alloc(layout) as *mut Node<T>;
            
            loop {
                let head = self.free_list.load(Ordering::Acquire);
                (*ptr).next = head;
                
                match self.free_list.compare_exchange_weak(
                    head, ptr, Ordering::Release, Ordering::Relaxed
                ) {
                    Ok(_) => {
                        self.allocated.fetch_sub(1, Ordering::Relaxed);
                        break;
                    }
                    Err(_) => continue,
                }
            }
        }
    }
}
```

### 2. Cache-Optimized Ring Buffer

Create `/src/performance/ring_buffer.rs`:

```rust
//! Cache-optimized ring buffer for assessment history

use std::sync::atomic::{AtomicUsize, Ordering};
use std::mem::MaybeUninit;

/// Lock-free ring buffer optimized for single producer, single consumer
#[repr(align(64))] // Cache line aligned
pub struct HftRingBuffer<T> {
    data: Box<[MaybeUninit<T>]>,
    capacity: usize,
    head: AtomicUsize,
    tail: AtomicUsize,
    _padding: [u8; 64 - 16], // Prevent false sharing
}

impl<T: Copy> HftRingBuffer<T> {
    pub fn new(capacity: usize) -> Self {
        assert!(capacity.is_power_of_two(), "Capacity must be power of 2 for performance");
        
        let mut data = Vec::with_capacity(capacity);
        data.resize_with(capacity, || MaybeUninit::uninit());
        
        Self {
            data: data.into_boxed_slice(),
            capacity,
            head: AtomicUsize::new(0),
            tail: AtomicUsize::new(0),
            _padding: [0; 64 - 16],
        }
    }
    
    /// Push element (lock-free, ~3ns typical)
    #[inline(always)]
    pub fn push(&self, item: T) -> bool {
        let head = self.head.load(Ordering::Relaxed);
        let next_head = (head + 1) & (self.capacity - 1); // Fast modulo for power of 2
        
        if next_head == self.tail.load(Ordering::Acquire) {
            return false; // Buffer full
        }
        
        unsafe {
            self.data[head].as_mut_ptr().write(item);
        }
        
        self.head.store(next_head, Ordering::Release);
        true
    }
    
    /// Pop element (lock-free, ~2ns typical)
    #[inline(always)]
    pub fn pop(&self) -> Option<T> {
        let tail = self.tail.load(Ordering::Relaxed);
        
        if tail == self.head.load(Ordering::Acquire) {
            return None; // Buffer empty
        }
        
        let item = unsafe { self.data[tail].as_ptr().read() };
        
        self.tail.store((tail + 1) & (self.capacity - 1), Ordering::Release);
        Some(item)
    }
    
    /// Get last N elements without popping (for analysis)
    #[inline(always)]
    pub fn last_n(&self, n: usize) -> Vec<T> {
        let head = self.head.load(Ordering::Acquire);
        let tail = self.tail.load(Ordering::Relaxed);
        
        let size = if head >= tail { head - tail } else { self.capacity - tail + head };
        let count = n.min(size);
        
        let mut result = Vec::with_capacity(count);
        for i in 0..count {
            let idx = (head - 1 - i) & (self.capacity - 1);
            unsafe {
                result.push(self.data[idx].as_ptr().read());
            }
        }
        
        result
    }
}
```

### 3. SIMD-Optimized Statistical Calculations

Optimize `/src/performance.rs` with specialized functions:

```rust
//! SIMD-optimized statistical calculations for HFT

#[cfg(target_arch = "x86_64")]
use std::arch::x86_64::*;

/// Calculate mean, variance, skewness, kurtosis in single SIMD pass
#[cfg(target_feature = "avx2")]
#[inline(always)]
pub unsafe fn statistics_avx2(data: &[f64]) -> StatisticsResult {
    if data.len() < 8 {
        return statistics_scalar(data);
    }
    
    let len = data.len();
    let n = len as f64;
    
    // Initialize SIMD accumulators
    let mut sum = _mm256_setzero_pd();
    let mut sum_sq = _mm256_setzero_pd();
    let mut min_vals = _mm256_set1_pd(f64::INFINITY);
    let mut max_vals = _mm256_set1_pd(f64::NEG_INFINITY);
    
    // First pass: sum, sum of squares, min, max
    let mut i = 0;
    while i + 4 <= len {
        let values = _mm256_loadu_pd(data.as_ptr().add(i));
        
        sum = _mm256_add_pd(sum, values);
        sum_sq = _mm256_fmadd_pd(values, values, sum_sq);
        min_vals = _mm256_min_pd(min_vals, values);
        max_vals = _mm256_max_pd(max_vals, values);
        
        i += 4;
    }
    
    // Handle remaining elements
    for j in i..len {
        let val = data[j];
        sum = _mm256_add_pd(sum, _mm256_set1_pd(val));
        sum_sq = _mm256_fmadd_pd(_mm256_set1_pd(val), _mm256_set1_pd(val), sum_sq);
    }
    
    // Extract results
    let total_sum = horizontal_sum(sum);
    let total_sum_sq = horizontal_sum(sum_sq);
    let min_val = horizontal_min(min_vals);
    let max_val = horizontal_max(max_vals);
    
    let mean = total_sum / n;
    let variance = (total_sum_sq - total_sum * total_sum / n) / (n - 1.0);
    
    // Second pass for higher moments (skewness, kurtosis)
    let std_dev = variance.sqrt();
    let mean_vec = _mm256_set1_pd(mean);
    let inv_std = _mm256_set1_pd(1.0 / std_dev);
    
    let mut sum_cubed = _mm256_setzero_pd();
    let mut sum_fourth = _mm256_setzero_pd();
    
    i = 0;
    while i + 4 <= len {
        let values = _mm256_loadu_pd(data.as_ptr().add(i));
        let normalized = _mm256_mul_pd(_mm256_sub_pd(values, mean_vec), inv_std);
        
        let squared = _mm256_mul_pd(normalized, normalized);
        let cubed = _mm256_mul_pd(squared, normalized);
        let fourth = _mm256_mul_pd(cubed, normalized);
        
        sum_cubed = _mm256_add_pd(sum_cubed, cubed);
        sum_fourth = _mm256_add_pd(sum_fourth, fourth);
        
        i += 4;
    }
    
    let skewness = horizontal_sum(sum_cubed) / n;
    let kurtosis = horizontal_sum(sum_fourth) / n - 3.0; // Excess kurtosis
    
    StatisticsResult {
        mean,
        variance,
        std_dev,
        skewness,
        kurtosis,
        min: min_val,
        max: max_val,
    }
}

/// Horizontal sum of AVX2 register
#[cfg(target_feature = "avx2")]
#[inline(always)]
unsafe fn horizontal_sum(v: __m256d) -> f64 {
    let hi = _mm256_extractf128_pd(v, 1);
    let lo = _mm256_castpd256_pd128(v);
    let sum_quad = _mm_add_pd(hi, lo);
    let hi64 = _mm_unpackhi_pd(sum_quad, sum_quad);
    let sum_final = _mm_add_sd(sum_quad, hi64);
    _mm_cvtsd_f64(sum_final)
}

#[derive(Debug, Clone)]
pub struct StatisticsResult {
    pub mean: f64,
    pub variance: f64,
    pub std_dev: f64,
    pub skewness: f64,
    pub kurtosis: f64,
    pub min: f64,
    pub max: f64,
}
```

### 4. Incremental Statistics for Black Swan Detection

Optimize `/src/black_swan.rs`:

```rust
//! Incremental statistics for O(1) updates

/// Welford's online algorithm for incremental statistics
#[derive(Debug, Clone)]
pub struct IncrementalStats {
    count: u64,
    mean: f64,
    m2: f64,    // For variance
    m3: f64,    // For skewness  
    m4: f64,    // For kurtosis
    min: f64,
    max: f64,
    sum: f64,   // For convenience
}

impl IncrementalStats {
    pub fn new() -> Self {
        Self {
            count: 0,
            mean: 0.0,
            m2: 0.0,
            m3: 0.0,
            m4: 0.0,
            min: f64::INFINITY,
            max: f64::NEG_INFINITY,
            sum: 0.0,
        }
    }
    
    /// Update statistics with new value (O(1) operation, ~5ns)
    #[inline(always)]
    pub fn update(&mut self, value: f64) {
        self.count += 1;
        self.sum += value;
        self.min = self.min.min(value);
        self.max = self.max.max(value);
        
        let n = self.count as f64;
        let delta = value - self.mean;
        let delta_n = delta / n;
        let delta_n2 = delta_n * delta_n;
        
        let term1 = delta * delta_n * (n - 1.0);
        
        self.mean += delta_n;
        self.m4 += term1 * delta_n2 * (n * n - 3.0 * n + 3.0) + 6.0 * delta_n2 * self.m2 - 4.0 * delta_n * self.m3;
        self.m3 += term1 * delta_n * (n - 2.0) - 3.0 * delta_n * self.m2;
        self.m2 += term1;
    }
    
    #[inline(always)]
    pub fn variance(&self) -> f64 {
        if self.count < 2 { 0.0 } else { self.m2 / (self.count - 1) as f64 }
    }
    
    #[inline(always)]
    pub fn std_dev(&self) -> f64 {
        self.variance().sqrt()
    }
    
    #[inline(always)]
    pub fn skewness(&self) -> f64 {
        if self.count < 3 || self.m2 == 0.0 { 
            0.0 
        } else { 
            (self.count as f64).sqrt() * self.m3 / self.m2.powf(1.5) 
        }
    }
    
    #[inline(always)]
    pub fn kurtosis(&self) -> f64 {
        if self.count < 4 || self.m2 == 0.0 { 
            0.0 
        } else { 
            (self.count as f64) * self.m4 / (self.m2 * self.m2) - 3.0 
        }
    }
}

// Update BlackSwanDetector to use incremental stats
impl BlackSwanDetector {
    /// Update with O(1) incremental statistics instead of O(n) recalculation
    fn update_baseline_statistics_incremental(&mut self, new_return: f64) -> Result<(), Box<dyn std::error::Error>> {
        self.incremental_stats.update(new_return);
        
        // Update baseline statistics from incremental calculations
        self.baseline_statistics = Some(BaselineStatistics {
            mean_return: self.incremental_stats.mean,
            std_dev: self.incremental_stats.std_dev(),
            volatility: self.incremental_stats.std_dev() * (252.0_f64).sqrt(),
            skewness: self.incremental_stats.skewness(),
            kurtosis: self.incremental_stats.kurtosis(),
            last_updated: Utc::now(),
        });
        
        Ok(())
    }
}
```

### 5. Lock-Free Quantum State Pool

Optimize `/src/quantum_antifragility.rs`:

```rust
//! Lock-free quantum state management

use crate::performance::HftMemoryPool;
use once_cell::sync::Lazy;

// Global quantum state pool for zero-allocation hot paths
static QUANTUM_STATE_POOL: Lazy<HftMemoryPool<QuantumState>> = 
    Lazy::new(|| HftMemoryPool::new(1000));

impl QuantumTalebianRisk {
    /// Get quantum state from pool (lock-free, <10ns)
    #[inline(always)]
    fn get_pooled_quantum_state(&self) -> Box<QuantumState> {
        QUANTUM_STATE_POOL.acquire()
            .unwrap_or_else(|| Box::new(QuantumState::new_uninitialized(self.config.num_qubits)))
    }
    
    /// Return quantum state to pool
    #[inline(always)]
    fn return_quantum_state(&self, state: Box<QuantumState>) {
        // Reset state for reuse
        state.reset();
        QUANTUM_STATE_POOL.release(state);
    }
    
    /// Optimized market data encoding with pooled states
    fn encode_market_data_optimized(&self, market_data: &[f64]) -> QuantumResult<Box<QuantumState>> {
        let mut quantum_state = self.get_pooled_quantum_state();
        
        // Use lookup table for tanh calculations
        for (i, &data_point) in market_data.iter().enumerate().take(self.config.num_qubits) {
            let amplitude = Complex64::new(fast_tanh(data_point), 0.0);
            quantum_state.set_amplitude_unchecked(i, amplitude);
        }
        
        quantum_state.normalize_inplace();
        Ok(quantum_state)
    }
}

// Fast tanh approximation using lookup table
const TANH_TABLE_SIZE: usize = 2048;
const TANH_TABLE: [f64; TANH_TABLE_SIZE] = generate_tanh_table();

const fn generate_tanh_table() -> [f64; TANH_TABLE_SIZE] {
    let mut table = [0.0; TANH_TABLE_SIZE];
    let mut i = 0;
    while i < TANH_TABLE_SIZE {
        let x = (i as f64 / TANH_TABLE_SIZE as f64) * 10.0 - 5.0; // Range [-5, 5]
        table[i] = fast_tanh_impl(x);
        i += 1;
    }
    table
}

const fn fast_tanh_impl(x: f64) -> f64 {
    // Rational approximation of tanh for compile-time evaluation
    if x > 5.0 { return 1.0; }
    if x < -5.0 { return -1.0; }
    let x2 = x * x;
    x * (27.0 + x2) / (27.0 + 9.0 * x2)
}

#[inline(always)]
fn fast_tanh(x: f64) -> f64 {
    if x >= 5.0 { return 1.0; }
    if x <= -5.0 { return -1.0; }
    
    let index = ((x + 5.0) * TANH_TABLE_SIZE as f64 / 10.0) as usize;
    TANH_TABLE[index.min(TANH_TABLE_SIZE - 1)]
}
```

### 6. Optimized Risk Engine with Parallel Components

Update `/src/risk_engine.rs`:

```rust
//! Parallel risk assessment with SIMD optimization

use rayon::prelude::*;
use crate::performance::{HftRingBuffer, StatisticsResult, statistics_avx2};

impl TalebianRiskEngine {
    /// Parallel risk assessment with SIMD optimization
    pub fn assess_risk_optimized(&mut self, market_data: &MarketData) -> Result<TalebianRiskAssessment, TalebianRiskError> {
        // Parallel execution of independent components
        let (whale_result, antifragility_result, base_black_swan) = rayon::join3(
            || self.whale_engine.detect_whale_activity(market_data),
            || self.antifragility_engine.calculate_antifragility_simd(market_data),
            || self.black_swan_engine.assess_base_risk_incremental(market_data)
        );
        
        let whale_detection = whale_result?;
        let antifragility = antifragility_result?;
        let black_swan_base = base_black_swan?;
        
        // Now calculate dependent components
        let (opportunity_result, barbell_result, kelly_result) = rayon::join3(
            || self.opportunity_engine.analyze_opportunity_vectorized(
                market_data, &whale_detection, antifragility.antifragility_score
            ),
            || self.barbell_engine.calculate_optimal_allocation_simd(
                market_data, &whale_detection, antifragility.antifragility_score
            ),
            || self.kelly_engine.calculate_kelly_fraction_fast(
                market_data, &whale_detection, 0.05, antifragility.confidence
            )
        );
        
        let opportunity = opportunity_result?;
        let barbell = barbell_result?;
        let kelly = kelly_result?;
        
        // Final black swan assessment with dependency
        let black_swan = self.black_swan_engine.finalize_assessment(black_swan_base, &whale_detection)?;
        
        // Vectorized score calculations
        let overall_risk_score = self.calculate_risk_score_simd(
            &antifragility, &black_swan, &opportunity, &whale_detection
        )?;
        
        let recommended_position_size = self.calculate_position_size_vectorized(
            &kelly, &barbell, &opportunity, &whale_detection
        )?;
        
        let confidence = self.calculate_confidence_fast(
            &antifragility, &black_swan, &opportunity, &whale_detection
        )?;
        
        let assessment = TalebianRiskAssessment {
            antifragility_score: antifragility.antifragility_score,
            barbell_allocation: (barbell.safe_allocation, barbell.risky_allocation),
            black_swan_probability: black_swan.swan_probability,
            kelly_fraction: kelly.adjusted_fraction,
            whale_detection,
            parasitic_opportunity: crate::ParasiticOpportunity {
                opportunity_score: opportunity.overall_score,
                momentum_factor: opportunity.momentum_component,
                volatility_factor: opportunity.volatility_component,
                whale_alignment: opportunity.whale_alignment_component,
                regime_factor: opportunity.regime_component,
                recommended_allocation: opportunity.recommended_allocation,
                confidence: opportunity.confidence,
            },
            overall_risk_score,
            recommended_position_size,
            confidence,
        };
        
        // Use lock-free ring buffer instead of VecDeque
        if !self.assessment_ring_buffer.push(assessment.clone()) {
            // Buffer full, this is expected in high-frequency scenarios
            // The oldest assessment is automatically dropped
        }
        
        // Update performance tracking (lock-free)
        self.update_performance_tracking_lockfree(&assessment)?;
        
        Ok(assessment)
    }
    
    /// SIMD-optimized risk score calculation
    #[inline(always)]
    fn calculate_risk_score_simd(
        &self,
        antifragility: &AntifragilityAssessment,
        black_swan: &BlackSwanAssessment,
        opportunity: &OpportunityAnalysis,
        whale_detection: &WhaleDetection
    ) -> Result<f64, TalebianRiskError> {
        
        // Vectorized weight calculation
        let scores = [
            antifragility.antifragility_score,
            1.0 - black_swan.swan_probability,
            opportunity.overall_score,
            if whale_detection.is_whale_detected { whale_detection.confidence } else { 0.3 }
        ];
        
        let weights = [0.25, 0.20, 0.30, 0.25];
        
        #[cfg(target_feature = "avx2")]
        unsafe {
            use std::arch::x86_64::*;
            let score_vec = _mm256_loadu_pd(scores.as_ptr());
            let weight_vec = _mm256_loadu_pd(weights.as_ptr());
            let weighted = _mm256_mul_pd(score_vec, weight_vec);
            
            let sum = crate::performance::horizontal_sum(weighted);
            Ok(sum.max(0.0).min(1.0))
        }
        
        #[cfg(not(target_feature = "avx2"))]
        {
            let weighted_sum: f64 = scores.iter().zip(weights.iter())
                .map(|(s, w)| s * w)
                .sum();
            Ok(weighted_sum.max(0.0).min(1.0))
        }
    }
}
```

---

## Build Configuration for Maximum Performance

Update `Cargo.toml`:

```toml
[package]
name = "talebian-risk-rs"
version = "0.2.0"
edition = "2021"

[dependencies]
# Core dependencies
nalgebra = { version = "0.32", features = ["serde-serialize"] }
ndarray = { version = "0.15", features = ["rayon", "blas"] }
rayon = "1.8"
once_cell = "1.19"

# SIMD and performance
wide = { version = "0.7", features = ["serde"] }
aligned-vec = "0.5"

# Lock-free data structures
lockfree = "0.5"
crossbeam = "0.8"

# Other dependencies...
rand = "0.8"
rand_distr = "0.4"
serde = { version = "1.0", features = ["derive"] }
chrono = { version = "0.4", features = ["serde"] }

[features]
default = ["simd", "parallel", "lockfree"]
simd = ["wide"]
parallel = ["rayon"]
lockfree = ["lockfree", "crossbeam"]
hft-optimized = ["simd", "parallel", "lockfree"]

[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
panic = "abort"
overflow-checks = false
debug = false

# CPU-specific optimizations
[profile.release-native]
inherits = "release"
rustflags = [
    "-C", "target-cpu=native",
    "-C", "target-feature=+avx2,+fma,+bmi2",
]

# For benchmarking
[profile.bench]
inherits = "release"
debug = true
```

Create `.cargo/config.toml`:

```toml
[build]
rustflags = [
    "-C", "target-cpu=native",
    "-C", "target-feature=+avx2,+fma",
    "-C", "force-frame-pointers=yes",
]

[target.x86_64-unknown-linux-gnu]
rustflags = [
    "-C", "link-arg=-fuse-ld=lld",  # Faster linker
    "-C", "target-cpu=native",
]
```

---

## Performance Testing and Validation

Create `/benches/hft_performance.rs`:

```rust
//! High-frequency trading performance benchmarks

use criterion::{black_box, criterion_group, criterion_main, Criterion, BatchSize};
use talebian_risk_rs::{MacchiavelianConfig, TalebianRiskEngine, MarketData};
use std::time::Duration;

fn bench_sub_millisecond_requirement(c: &mut Criterion) {
    let config = MacchiavelianConfig::aggressive_defaults();
    let mut engine = TalebianRiskEngine::new(config);
    let market_data = create_realistic_market_data();
    
    let mut group = c.benchmark_group("hft_latency");
    group.significance_level(0.1);
    group.sample_size(10000);
    group.measurement_time(Duration::from_secs(30));
    
    // Target: <250Œºs average, <500Œºs 95th percentile
    group.bench_function("risk_assessment", |b| {
        b.iter_batched(
            || market_data.clone(),
            |data| {
                black_box(engine.assess_risk_optimized(black_box(&data)).unwrap())
            },
            BatchSize::SmallInput
        );
    });
    
    group.finish();
}

fn bench_throughput_hft(c: &mut Criterion) {
    let config = MacchiavelianConfig::aggressive_defaults();
    let mut engine = TalebianRiskEngine::new(config);
    
    let mut group = c.benchmark_group("hft_throughput");
    
    // Test sustained throughput
    for batch_size in [10, 100, 1000] {
        let market_data_batch: Vec<_> = (0..batch_size)
            .map(|i| create_realistic_market_data_with_variance(i))
            .collect();
        
        group.bench_with_input(
            criterion::BenchmarkId::new("batch_processing", batch_size),
            &batch_size,
            |b, _| {
                b.iter(|| {
                    for data in &market_data_batch {
                        black_box(engine.assess_risk_optimized(black_box(data)).unwrap());
                    }
                });
            },
        );
    }
    
    group.finish();
}

fn bench_memory_allocation_overhead(c: &mut Criterion) {
    let config = MacchiavelianConfig::aggressive_defaults();
    let mut engine = TalebianRiskEngine::new(config);
    let market_data = create_realistic_market_data();
    
    // Measure allocator performance
    c.bench_function("memory_pool_allocation", |b| {
        b.iter(|| {
            // This should use pooled allocations
            black_box(engine.assess_risk_optimized(black_box(&market_data)).unwrap());
        });
    });
}

criterion_group!(
    hft_benches,
    bench_sub_millisecond_requirement,
    bench_throughput_hft,
    bench_memory_allocation_overhead
);

criterion_main!(hft_benches);
```

---

## Performance Validation Script

Create `/scripts/validate_performance.sh`:

```bash
#!/bin/bash
# Performance validation for HFT requirements

set -e

echo "üöÄ High-Frequency Trading Performance Validation"
echo "================================================"

# Build with maximum optimizations
echo "üì¶ Building with HFT optimizations..."
RUSTFLAGS="-C target-cpu=native -C target-feature=+avx2,+fma" \
    cargo build --release --features=hft-optimized

# Run benchmarks
echo "‚ö° Running latency benchmarks..."
cargo bench --bench hft_performance 2>&1 | tee benchmark_results.txt

# Validate latency requirements
echo "üìä Validating latency requirements..."
python3 scripts/analyze_latency.py benchmark_results.txt

# Profile with perf
echo "üîç CPU profiling..."
sudo perf record --call-graph=dwarf \
    target/release/examples/hft_example

sudo perf report --no-children --sort=symbol > perf_report.txt

# Memory profiling
echo "üíæ Memory profiling..."
valgrind --tool=massif --time-unit=ms \
    target/release/examples/hft_example

# Validate results
echo "‚úÖ Performance validation complete!"
echo "   Target: <250Œºs average latency"
echo "   Target: <500Œºs 95th percentile"
echo "   Target: >4000 assessments/second"

grep -E "(mean|p95|p99)" benchmark_results.txt
```

Create `/scripts/analyze_latency.py`:

```python
#!/usr/bin/env python3
"""Analyze benchmark results for HFT compliance"""

import sys
import re
from statistics import mean, median

def parse_benchmark_results(filename):
    with open(filename, 'r') as f:
        content = f.read()
    
    # Extract latency measurements
    latency_pattern = r'time:\s*\[([0-9.]+)\s*([a-z]+)'
    matches = re.findall(latency_pattern, content)
    
    latencies = []
    for value, unit in matches:
        val = float(value)
        if unit == 'ns':
            val /= 1000  # Convert to microseconds
        elif unit == 'ms':
            val *= 1000  # Convert to microseconds
        latencies.append(val)
    
    return latencies

def main():
    if len(sys.argv) != 2:
        print("Usage: analyze_latency.py <benchmark_results.txt>")
        sys.exit(1)
    
    latencies = parse_benchmark_results(sys.argv[1])
    
    if not latencies:
        print("‚ùå No latency data found in benchmark results")
        sys.exit(1)
    
    avg_latency = mean(latencies)
    median_latency = median(latencies)
    p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
    p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
    
    print(f"üìä Latency Analysis Results:")
    print(f"   Average: {avg_latency:.1f}Œºs")
    print(f"   Median:  {median_latency:.1f}Œºs")
    print(f"   P95:     {p95_latency:.1f}Œºs")
    print(f"   P99:     {p99_latency:.1f}Œºs")
    
    # HFT requirements validation
    requirements_met = True
    
    if avg_latency > 250:
        print(f"‚ùå Average latency {avg_latency:.1f}Œºs exceeds 250Œºs target")
        requirements_met = False
    else:
        print(f"‚úÖ Average latency {avg_latency:.1f}Œºs meets <250Œºs target")
    
    if p95_latency > 500:
        print(f"‚ùå P95 latency {p95_latency:.1f}Œºs exceeds 500Œºs target")
        requirements_met = False
    else:
        print(f"‚úÖ P95 latency {p95_latency:.1f}Œºs meets <500Œºs target")
    
    if p99_latency > 1000:
        print(f"‚ö†Ô∏è  P99 latency {p99_latency:.1f}Œºs exceeds 1ms (warning)")
    else:
        print(f"‚úÖ P99 latency {p99_latency:.1f}Œºs acceptable")
    
    if requirements_met:
        print("\nüéâ All HFT latency requirements met!")
        sys.exit(0)
    else:
        print("\nüö® HFT latency requirements NOT met!")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## Implementation Timeline

### Week 1: Core Optimizations
- [ ] Implement HftRingBuffer for assessment history
- [ ] Create HftMemoryPool for quantum states
- [ ] Enable SIMD features in Cargo.toml
- [ ] Basic performance testing

### Week 2: SIMD Integration
- [ ] Implement statistics_avx2 function
- [ ] Optimize whale detection calculations
- [ ] Add vectorized risk scoring
- [ ] Benchmark SIMD improvements

### Week 3: Lock-Free Structures
- [ ] Implement incremental statistics
- [ ] Add lock-free performance monitoring
- [ ] Optimize parallel component execution
- [ ] Full system integration testing

### Week 4: Validation & Tuning
- [ ] Complete performance validation
- [ ] CPU profiling and optimization
- [ ] Memory allocation analysis
- [ ] Production readiness testing

---

This implementation guide provides the concrete steps to achieve sub-millisecond latency for the talebian-risk-rs high-frequency trading system. The optimizations focus on the most impactful changes: memory allocation elimination, SIMD vectorization, and lock-free data structures.