// GPU Pipeline Acceleration System
// High-performance GPU-accelerated trading pipeline with real-time quantum processing

use std::sync::Arc;
use std::collections::HashMap;
use std::time::{Duration, Instant};
use tokio::sync::{RwLock, mpsc, Mutex};
use anyhow::{Result, anyhow};
use serde::{Deserialize, Serialize};
use futures::stream::{self, StreamExt};

// Core GPU-accelerated pipeline components
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GPUPipelineConfig {
    pub batch_size: usize,
    pub max_concurrent_operations: usize,
    pub memory_pool_size_mb: usize,
    pub gpu_device_id: usize,
    pub enable_multi_gpu: bool,
    pub enable_tensor_cores: bool,
    pub precision: PrecisionMode,
    pub streaming_enabled: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PrecisionMode {
    FP32,
    FP16,
    Mixed,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MarketData {
    pub symbol: String,
    pub timestamp: i64,
    pub price: f64,
    pub volume: f64,
    pub bid: f64,
    pub ask: f64,
    pub features: Vec<f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QuantumSignal {
    pub signal_type: SignalType,
    pub strength: f64,
    pub confidence: f64,
    pub quantum_state: Vec<f64>,
    pub execution_time_us: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SignalType {
    Buy,
    Sell,
    Hold,
    StopLoss,
    TakeProfit,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TradingDecision {
    pub symbol: String,
    pub action: SignalType,
    pub quantity: f64,
    pub price: f64,
    pub risk_score: f64,
    pub quantum_confidence: f64,
    pub latency_us: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PipelineMetrics {
    pub total_processed: usize,
    pub average_latency_us: u64,
    pub throughput_ops_per_sec: f64,
    pub gpu_utilization: f64,
    pub memory_usage_mb: f64,
    pub quantum_accuracy: f64,
    pub error_rate: f64,
}

// GPU-accelerated trading pipeline
pub struct GPUTradingPipeline {
    config: GPUPipelineConfig,
    quantum_processor: Arc<GPUQuantumProcessor>,
    neural_engine: Arc<GPUNeuralEngine>,
    matrix_accelerator: Arc<GPUMatrixAccelerator>,
    memory_pool: Arc<GPUMemoryPool>,
    metrics: Arc<RwLock<PipelineMetrics>>,
    market_data_rx: Arc<Mutex<mpsc::Receiver<MarketData>>>,
    trading_decisions_tx: Arc<Mutex<mpsc::Sender<TradingDecision>>>,
}

impl GPUTradingPipeline {
    pub async fn new(
        config: GPUPipelineConfig,
        market_data_rx: mpsc::Receiver<MarketData>,
        trading_decisions_tx: mpsc::Sender<TradingDecision>,
    ) -> Result<Self> {
        let quantum_processor = Arc::new(GPUQuantumProcessor::new(&config).await?);
        let neural_engine = Arc::new(GPUNeuralEngine::new(&config).await?);
        let matrix_accelerator = Arc::new(GPUMatrixAccelerator::new(&config).await?);
        let memory_pool = Arc::new(GPUMemoryPool::new(&config).await?);
        
        Ok(Self {
            config,
            quantum_processor,
            neural_engine,
            matrix_accelerator,
            memory_pool,
            metrics: Arc::new(RwLock::new(PipelineMetrics::default())),
            market_data_rx: Arc::new(Mutex::new(market_data_rx)),
            trading_decisions_tx: Arc::new(Mutex::new(trading_decisions_tx)),
        })
    }

    pub async fn start_pipeline(&self) -> Result<()> {
        println!("ðŸš€ Starting GPU-accelerated trading pipeline...");
        
        // Initialize GPU resources
        self.quantum_processor.initialize().await?;
        self.neural_engine.initialize().await?;
        self.matrix_accelerator.initialize().await?;
        self.memory_pool.initialize().await?;
        
        // Start processing loop
        let pipeline_clone = Arc::new(self.clone());
        tokio::spawn(async move {
            if let Err(e) = pipeline_clone.processing_loop().await {
                eprintln!("Pipeline processing error: {}", e);
            }
        });
        
        println!("âœ… GPU pipeline started successfully!");
        Ok(())
    }

    async fn processing_loop(&self) -> Result<()> {
        let mut batch_buffer = Vec::new();
        let mut last_batch_time = Instant::now();
        
        loop {
            // Collect market data into batches
            let mut rx_guard = self.market_data_rx.lock().await;
            
            // Non-blocking receive to build batches
            match rx_guard.try_recv() {
                Ok(market_data) => {
                    batch_buffer.push(market_data);
                    
                    // Process batch when full or timeout reached
                    if batch_buffer.len() >= self.config.batch_size 
                        || last_batch_time.elapsed() > Duration::from_millis(10) {
                        
                        let batch = std::mem::take(&mut batch_buffer);
                        self.process_batch(batch).await?;
                        last_batch_time = Instant::now();
                    }
                }
                Err(mpsc::error::TryRecvError::Empty) => {
                    // Process partial batch if timeout reached
                    if !batch_buffer.is_empty() && last_batch_time.elapsed() > Duration::from_millis(5) {
                        let batch = std::mem::take(&mut batch_buffer);
                        self.process_batch(batch).await?;
                        last_batch_time = Instant::now();
                    }
                    
                    // Small delay to prevent busy waiting
                    tokio::time::sleep(Duration::from_microseconds(100)).await;
                }
                Err(mpsc::error::TryRecvError::Disconnected) => {
                    println!("Market data stream disconnected");
                    break;
                }
            }
            
            drop(rx_guard);
        }
        
        Ok(())
    }

    async fn process_batch(&self, batch: Vec<MarketData>) -> Result<()> {
        let batch_start = Instant::now();
        let batch_size = batch.len();
        
        // Stage 1: Feature extraction and preprocessing (GPU-accelerated)
        let features = self.extract_features_gpu(&batch).await?;
        
        // Stage 2: Quantum signal generation (GPU quantum circuits)
        let quantum_signals = self.generate_quantum_signals(&features).await?;
        
        // Stage 3: Neural network inference (GPU-accelerated)
        let neural_predictions = self.neural_engine.predict(&features).await?;
        
        // Stage 4: Matrix operations for portfolio optimization
        let portfolio_weights = self.matrix_accelerator.optimize_portfolio(&features, &quantum_signals).await?;
        
        // Stage 5: Risk assessment and decision making
        let trading_decisions = self.generate_trading_decisions(
            &batch,
            &quantum_signals,
            &neural_predictions,
            &portfolio_weights,
        ).await?;
        
        // Send decisions to trading engine
        let mut tx_guard = self.trading_decisions_tx.lock().await;
        for decision in trading_decisions {
            if let Err(e) = tx_guard.send(decision).await {
                eprintln!("Failed to send trading decision: {}", e);
            }
        }
        drop(tx_guard);
        
        // Update metrics
        let batch_latency = batch_start.elapsed();
        self.update_metrics(batch_size, batch_latency).await;
        
        Ok(())
    }

    async fn extract_features_gpu(&self, batch: &[MarketData]) -> Result<Vec<Vec<f64>>> {
        // GPU-accelerated feature extraction
        let start_time = Instant::now();
        
        // Simulate GPU feature extraction
        let features = stream::iter(batch)
            .map(|market_data| async {
                // GPU-accelerated technical indicators
                let mut features = Vec::new();
                
                // Price features
                features.push(market_data.price);
                features.push(market_data.volume);
                features.push(market_data.bid);
                features.push(market_data.ask);
                features.push(market_data.ask - market_data.bid); // Spread
                
                // GPU-accelerated moving averages
                features.extend(self.calculate_moving_averages_gpu(market_data).await);
                
                // GPU-accelerated momentum indicators
                features.extend(self.calculate_momentum_indicators_gpu(market_data).await);
                
                // GPU-accelerated volatility measures
                features.extend(self.calculate_volatility_gpu(market_data).await);
                
                features
            })
            .buffer_unordered(self.config.max_concurrent_operations)
            .collect::<Vec<_>>()
            .await;
        
        let extraction_time = start_time.elapsed();
        println!("âš¡ GPU feature extraction: {}Î¼s", extraction_time.as_micros());
        
        Ok(features)
    }

    async fn generate_quantum_signals(&self, features: &[Vec<f64>]) -> Result<Vec<QuantumSignal>> {
        self.quantum_processor.process_batch(features).await
    }

    async fn generate_trading_decisions(
        &self,
        batch: &[MarketData],
        quantum_signals: &[QuantumSignal],
        neural_predictions: &[f64],
        portfolio_weights: &[f64],
    ) -> Result<Vec<TradingDecision>> {
        let mut decisions = Vec::new();
        
        for (i, market_data) in batch.iter().enumerate() {
            let quantum_signal = &quantum_signals[i];
            let neural_prediction = neural_predictions[i];
            let portfolio_weight = portfolio_weights[i];
            
            // Combine quantum and neural signals
            let combined_strength = quantum_signal.strength * 0.6 + neural_prediction * 0.4;
            let combined_confidence = quantum_signal.confidence * portfolio_weight;
            
            // Generate trading decision
            let action = match combined_strength {
                s if s > 0.7 => SignalType::Buy,
                s if s < -0.7 => SignalType::Sell,
                _ => SignalType::Hold,
            };
            
            let decision = TradingDecision {
                symbol: market_data.symbol.clone(),
                action,
                quantity: portfolio_weight * 100.0, // Position size based on portfolio weight
                price: market_data.price,
                risk_score: 1.0 - combined_confidence,
                quantum_confidence: quantum_signal.confidence,
                latency_us: quantum_signal.execution_time_us,
            };
            
            decisions.push(decision);
        }
        
        Ok(decisions)
    }

    async fn update_metrics(&self, batch_size: usize, batch_latency: Duration) {
        let mut metrics = self.metrics.write().await;
        
        metrics.total_processed += batch_size;
        metrics.average_latency_us = (metrics.average_latency_us + batch_latency.as_micros() as u64) / 2;
        metrics.throughput_ops_per_sec = batch_size as f64 / batch_latency.as_secs_f64();
        metrics.gpu_utilization = 85.0; // Mock GPU utilization
        metrics.memory_usage_mb = 8192.0; // Mock memory usage
        metrics.quantum_accuracy = 0.95; // Mock quantum accuracy
        metrics.error_rate = 0.01; // Mock error rate
    }

    // GPU-accelerated feature calculation methods
    async fn calculate_moving_averages_gpu(&self, market_data: &MarketData) -> Vec<f64> {
        // Mock GPU-accelerated moving averages
        vec![
            market_data.price * 0.95, // MA_5
            market_data.price * 0.90, // MA_10
            market_data.price * 0.85, // MA_20
        ]
    }

    async fn calculate_momentum_indicators_gpu(&self, market_data: &MarketData) -> Vec<f64> {
        // Mock GPU-accelerated momentum indicators
        vec![
            0.55, // RSI
            0.2,  // MACD
            0.3,  // Stochastic
        ]
    }

    async fn calculate_volatility_gpu(&self, market_data: &MarketData) -> Vec<f64> {
        // Mock GPU-accelerated volatility measures
        vec![
            0.25, // ATR
            0.15, // Bollinger Band Width
            0.35, // Volatility Index
        ]
    }

    pub async fn get_metrics(&self) -> PipelineMetrics {
        self.metrics.read().await.clone()
    }

    pub async fn shutdown(&self) -> Result<()> {
        println!("ðŸ›‘ Shutting down GPU pipeline...");
        
        self.quantum_processor.shutdown().await?;
        self.neural_engine.shutdown().await?;
        self.matrix_accelerator.shutdown().await?;
        self.memory_pool.shutdown().await?;
        
        println!("âœ… GPU pipeline shutdown complete");
        Ok(())
    }
}

impl Clone for GPUTradingPipeline {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            quantum_processor: self.quantum_processor.clone(),
            neural_engine: self.neural_engine.clone(),
            matrix_accelerator: self.matrix_accelerator.clone(),
            memory_pool: self.memory_pool.clone(),
            metrics: self.metrics.clone(),
            market_data_rx: self.market_data_rx.clone(),
            trading_decisions_tx: self.trading_decisions_tx.clone(),
        }
    }
}

// GPU Quantum Processor
pub struct GPUQuantumProcessor {
    config: GPUPipelineConfig,
    quantum_circuits: HashMap<String, QuantumCircuit>,
    gpu_context: Option<Arc<dyn GPUContext>>,
}

impl GPUQuantumProcessor {
    pub async fn new(config: &GPUPipelineConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            quantum_circuits: HashMap::new(),
            gpu_context: None,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        println!("ðŸ”¬ Initializing GPU quantum processor...");
        // Initialize quantum circuits and GPU context
        Ok(())
    }

    pub async fn process_batch(&self, features: &[Vec<f64>]) -> Result<Vec<QuantumSignal>> {
        let start_time = Instant::now();
        
        // GPU-accelerated quantum circuit processing
        let mut signals = Vec::new();
        
        for feature_set in features {
            let signal = self.process_single_quantum_circuit(feature_set).await?;
            signals.push(signal);
        }
        
        let processing_time = start_time.elapsed();
        println!("âš›ï¸ Quantum processing: {}Î¼s", processing_time.as_micros());
        
        Ok(signals)
    }

    async fn process_single_quantum_circuit(&self, features: &[f64]) -> Result<QuantumSignal> {
        // Mock quantum circuit processing
        let signal_strength = features.iter().sum::<f64>() / features.len() as f64;
        let confidence = 0.85 + (signal_strength * 0.1);
        
        let signal_type = if signal_strength > 0.6 {
            SignalType::Buy
        } else if signal_strength < 0.4 {
            SignalType::Sell
        } else {
            SignalType::Hold
        };
        
        Ok(QuantumSignal {
            signal_type,
            strength: signal_strength,
            confidence,
            quantum_state: vec![0.7, 0.3, 0.5, 0.8], // Mock quantum state
            execution_time_us: 250,
        })
    }

    pub async fn shutdown(&self) -> Result<()> {
        println!("ðŸ”¬ Shutting down quantum processor...");
        Ok(())
    }
}

// GPU Neural Engine
pub struct GPUNeuralEngine {
    config: GPUPipelineConfig,
    models: HashMap<String, NeuralModel>,
    gpu_context: Option<Arc<dyn GPUContext>>,
}

impl GPUNeuralEngine {
    pub async fn new(config: &GPUPipelineConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            models: HashMap::new(),
            gpu_context: None,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        println!("ðŸ§  Initializing GPU neural engine...");
        Ok(())
    }

    pub async fn predict(&self, features: &[Vec<f64>]) -> Result<Vec<f64>> {
        let start_time = Instant::now();
        
        // GPU-accelerated neural network inference
        let predictions = features
            .iter()
            .map(|feature_set| {
                // Mock neural network prediction
                let sum = feature_set.iter().sum::<f64>();
                (sum / feature_set.len() as f64).tanh()
            })
            .collect();
        
        let inference_time = start_time.elapsed();
        println!("ðŸ§  Neural inference: {}Î¼s", inference_time.as_micros());
        
        Ok(predictions)
    }

    pub async fn shutdown(&self) -> Result<()> {
        println!("ðŸ§  Shutting down neural engine...");
        Ok(())
    }
}

// GPU Matrix Accelerator
pub struct GPUMatrixAccelerator {
    config: GPUPipelineConfig,
    gpu_context: Option<Arc<dyn GPUContext>>,
}

impl GPUMatrixAccelerator {
    pub async fn new(config: &GPUPipelineConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            gpu_context: None,
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        println!("ðŸ”¢ Initializing GPU matrix accelerator...");
        Ok(())
    }

    pub async fn optimize_portfolio(
        &self,
        features: &[Vec<f64>],
        quantum_signals: &[QuantumSignal],
    ) -> Result<Vec<f64>> {
        let start_time = Instant::now();
        
        // GPU-accelerated portfolio optimization
        let weights = features
            .iter()
            .zip(quantum_signals.iter())
            .map(|(feature_set, signal)| {
                // Mock portfolio optimization
                let risk_adjustment = 1.0 - signal.strength.abs() * 0.1;
                let base_weight = signal.confidence * risk_adjustment;
                base_weight.max(0.01).min(0.1) // Constrain weights
            })
            .collect();
        
        let optimization_time = start_time.elapsed();
        println!("ðŸ”¢ Portfolio optimization: {}Î¼s", optimization_time.as_micros());
        
        Ok(weights)
    }

    pub async fn shutdown(&self) -> Result<()> {
        println!("ðŸ”¢ Shutting down matrix accelerator...");
        Ok(())
    }
}

// GPU Memory Pool
pub struct GPUMemoryPool {
    config: GPUPipelineConfig,
    allocated_memory: Arc<RwLock<HashMap<String, usize>>>,
}

impl GPUMemoryPool {
    pub async fn new(config: &GPUPipelineConfig) -> Result<Self> {
        Ok(Self {
            config: config.clone(),
            allocated_memory: Arc::new(RwLock::new(HashMap::new())),
        })
    }

    pub async fn initialize(&self) -> Result<()> {
        println!("ðŸ’¾ Initializing GPU memory pool...");
        Ok(())
    }

    pub async fn shutdown(&self) -> Result<()> {
        println!("ðŸ’¾ Shutting down memory pool...");
        Ok(())
    }
}

// Supporting structures
#[derive(Debug, Clone)]
pub struct QuantumCircuit {
    pub name: String,
    pub gates: Vec<String>,
    pub qubits: usize,
}

#[derive(Debug, Clone)]
pub struct NeuralModel {
    pub name: String,
    pub layers: Vec<usize>,
    pub activation: String,
}

impl Default for PipelineMetrics {
    fn default() -> Self {
        Self {
            total_processed: 0,
            average_latency_us: 0,
            throughput_ops_per_sec: 0.0,
            gpu_utilization: 0.0,
            memory_usage_mb: 0.0,
            quantum_accuracy: 0.0,
            error_rate: 0.0,
        }
    }
}

// Mock GPU context trait
trait GPUContext: Send + Sync {
    fn device_count(&self) -> usize;
    fn synchronize(&self) -> Result<()>;
    fn get_memory_info(&self) -> Result<(usize, usize)>;
}

// Demo and testing
#[tokio::main]
async fn main() -> Result<()> {
    println!("ðŸš€ GPU Pipeline Acceleration Demo");
    println!("================================");
    
    // Configure GPU pipeline
    let config = GPUPipelineConfig {
        batch_size: 32,
        max_concurrent_operations: 16,
        memory_pool_size_mb: 8192,
        gpu_device_id: 0,
        enable_multi_gpu: false,
        enable_tensor_cores: true,
        precision: PrecisionMode::Mixed,
        streaming_enabled: true,
    };
    
    // Create channels for market data and trading decisions
    let (market_data_tx, market_data_rx) = mpsc::channel(1000);
    let (trading_decisions_tx, mut trading_decisions_rx) = mpsc::channel(1000);
    
    // Initialize GPU pipeline
    let pipeline = GPUTradingPipeline::new(config, market_data_rx, trading_decisions_tx).await?;
    
    // Start the pipeline
    pipeline.start_pipeline().await?;
    
    // Simulate market data stream
    let data_generator = tokio::spawn(async move {
        for i in 0..1000 {
            let market_data = MarketData {
                symbol: format!("SYMBOL_{}", i % 10),
                timestamp: chrono::Utc::now().timestamp_millis(),
                price: 100.0 + (i as f64 * 0.1),
                volume: 1000.0 + (i as f64 * 10.0),
                bid: 99.9 + (i as f64 * 0.1),
                ask: 100.1 + (i as f64 * 0.1),
                features: vec![0.5, 0.3, 0.7, 0.2, 0.8],
            };
            
            if let Err(e) = market_data_tx.send(market_data).await {
                eprintln!("Failed to send market data: {}", e);
                break;
            }
            
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    });
    
    // Collect trading decisions
    let decision_collector = tokio::spawn(async move {
        let mut decisions_count = 0;
        while let Some(decision) = trading_decisions_rx.recv().await {
            decisions_count += 1;
            if decisions_count % 100 == 0 {
                println!("ðŸ“Š Processed {} trading decisions", decisions_count);
                println!("   Latest: {:?} - {}Î¼s latency", decision.action, decision.latency_us);
            }
        }
    });
    
    // Run for a while then show metrics
    tokio::time::sleep(Duration::from_secs(30)).await;
    
    let metrics = pipeline.get_metrics().await;
    println!("\nðŸ“ˆ Pipeline Performance Metrics:");
    println!("   Total Processed: {}", metrics.total_processed);
    println!("   Average Latency: {}Î¼s", metrics.average_latency_us);
    println!("   Throughput: {:.1} ops/sec", metrics.throughput_ops_per_sec);
    println!("   GPU Utilization: {:.1}%", metrics.gpu_utilization);
    println!("   Memory Usage: {:.1}MB", metrics.memory_usage_mb);
    println!("   Quantum Accuracy: {:.2}%", metrics.quantum_accuracy * 100.0);
    println!("   Error Rate: {:.2}%", metrics.error_rate * 100.0);
    
    // Shutdown
    pipeline.shutdown().await?;
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_gpu_pipeline_initialization() {
        let config = GPUPipelineConfig {
            batch_size: 16,
            max_concurrent_operations: 8,
            memory_pool_size_mb: 4096,
            gpu_device_id: 0,
            enable_multi_gpu: false,
            enable_tensor_cores: true,
            precision: PrecisionMode::FP32,
            streaming_enabled: true,
        };
        
        let (_, market_data_rx) = mpsc::channel(100);
        let (trading_decisions_tx, _) = mpsc::channel(100);
        
        let pipeline = GPUTradingPipeline::new(config, market_data_rx, trading_decisions_tx).await.unwrap();
        assert_eq!(pipeline.config.batch_size, 16);
    }
    
    #[tokio::test]
    async fn test_feature_extraction_performance() {
        let config = GPUPipelineConfig {
            batch_size: 32,
            max_concurrent_operations: 16,
            memory_pool_size_mb: 8192,
            gpu_device_id: 0,
            enable_multi_gpu: false,
            enable_tensor_cores: true,
            precision: PrecisionMode::Mixed,
            streaming_enabled: true,
        };
        
        let (_, market_data_rx) = mpsc::channel(100);
        let (trading_decisions_tx, _) = mpsc::channel(100);
        
        let pipeline = GPUTradingPipeline::new(config, market_data_rx, trading_decisions_tx).await.unwrap();
        
        let batch = vec![
            MarketData {
                symbol: "TEST".to_string(),
                timestamp: 1234567890,
                price: 100.0,
                volume: 1000.0,
                bid: 99.9,
                ask: 100.1,
                features: vec![0.5, 0.3, 0.7],
            };
            10
        ];
        
        let start = Instant::now();
        let features = pipeline.extract_features_gpu(&batch).await.unwrap();
        let duration = start.elapsed();
        
        assert_eq!(features.len(), 10);
        assert!(duration < Duration::from_millis(10)); // Should be fast
    }
    
    #[tokio::test]
    async fn test_quantum_signal_generation() {
        let config = GPUPipelineConfig {
            batch_size: 16,
            max_concurrent_operations: 8,
            memory_pool_size_mb: 4096,
            gpu_device_id: 0,
            enable_multi_gpu: false,
            enable_tensor_cores: true,
            precision: PrecisionMode::FP32,
            streaming_enabled: true,
        };
        
        let quantum_processor = GPUQuantumProcessor::new(&config).await.unwrap();
        
        let features = vec![
            vec![0.5, 0.3, 0.7, 0.2, 0.8],
            vec![0.6, 0.4, 0.2, 0.9, 0.1],
        ];
        
        let signals = quantum_processor.process_batch(&features).await.unwrap();
        
        assert_eq!(signals.len(), 2);
        assert!(signals[0].confidence > 0.0);
        assert!(signals[0].execution_time_us > 0);
    }
    
    #[tokio::test]
    async fn test_neural_inference_performance() {
        let config = GPUPipelineConfig {
            batch_size: 32,
            max_concurrent_operations: 16,
            memory_pool_size_mb: 8192,
            gpu_device_id: 0,
            enable_multi_gpu: false,
            enable_tensor_cores: true,
            precision: PrecisionMode::Mixed,
            streaming_enabled: true,
        };
        
        let neural_engine = GPUNeuralEngine::new(&config).await.unwrap();
        
        let features = vec![
            vec![0.5, 0.3, 0.7, 0.2, 0.8],
            vec![0.6, 0.4, 0.2, 0.9, 0.1],
        ];
        
        let start = Instant::now();
        let predictions = neural_engine.predict(&features).await.unwrap();
        let duration = start.elapsed();
        
        assert_eq!(predictions.len(), 2);
        assert!(duration < Duration::from_millis(5)); // Should be very fast
        assert!(predictions[0] >= -1.0 && predictions[0] <= 1.0); // tanh output
    }
}